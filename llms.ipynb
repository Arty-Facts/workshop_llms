{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Language Model (LLM): \n",
    "A Large Language Model is an advanced type of artificial intelligence model designed to understand and generate human-like text. \n",
    "These models are trained on vast amounts of text data from diverse sources like books, websites, and articles.\n",
    "\n",
    "* **Context-Based:** LLMs work by analyzing the context in which words appear. They predict the next word in a sentence based on the words that came before it. This ability to understand and generate text based on context makes them incredibly powerful for a wide range of language-related tasks.\n",
    "* **Pre-Trained:** These models are pre-trained on large datasets, meaning they have already learned to recognize patterns, structures, and meanings in text. This pre-training allows them to be used for various tasks with minimal additional training or customization.\n",
    "* **Versatility:** LLMs can perform a wide variety of tasks, such as generating creative writing, answering questions, translating languages, summarizing text, and even understanding the sentiment behind a message.\n",
    "\n",
    "The best way to truly understand LLMs is to experiment with them yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, you'll need to install the required libraries. The Hugging Face transformers library makes it easy for working with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch \"numpy<2\" ipywidgets requests_html lxml_html_clean \"faiss-cpu<1.8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, AutoModel\n",
    "import torch, faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Generation\n",
    "Let's start with a simple text generation example using the pipeline API from Hugging Face.\n",
    "\n",
    "Note we are using a very small model so that its easier to run on CPU. This will inpact the quality of the generated text. \n",
    "However i prefer to find the lower bound of the model performance. Because if the model can generate good text with a small model, then it will be even better with a larger model. \n",
    "\n",
    "You can find more models at https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"HuggingFaceTB/SmolLM-360M\"\n",
    "\n",
    "device = \"cpu\" # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "# Display the maximum sequence length (context window size)\n",
    "\n",
    "def generated_text(input_text, tokenizer=tokenizer, model=model, device=device, config=config, max_new_tokens=100):\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    print(f\"Maximum sequence length: {config.max_position_embeddings} tokens, input is {inputs.shape[1]} tokens\")\n",
    "    attention_mask = torch.ones(inputs.shape, device=device)\n",
    "    outputs = model.generate(inputs, max_new_tokens=max_new_tokens, temperature=0.5, do_sample=True, \n",
    "                             pad_token_id=tokenizer.eos_token_id, mask_token_id=tokenizer.mask_token_id, \n",
    "                             eos_token_id=tokenizer.eos_token_id, attention_mask=attention_mask)\n",
    "    output = tokenizer.decode(outputs[0])\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 74 tokens\n",
      "\n",
      "User: whome do i speak to?\n",
      "Assistent: You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\n",
      "User: Can you give me a short summary of what Entiros does?\n",
      "Assistent: 1. We are a chatbot that helps people with their health and wellness. 2. We use artificial intelligence to analyze your health and wellness data and provide you with personalized recommendations. 3. We also provide you with health and wellness resources, such as articles and videos.\n",
      "User: How can I get started with Entiros?\n",
      "Assistent: 1. Visit the Entiros website and sign up for an account. 2. You can also find more information about Ent\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_text = \"\"\"\n",
    "User: whome do i speak to?\n",
    "Assistent: You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\n",
    "User: Can you give me a short summary of what Entiros does?\n",
    "Assistent: \"\"\"\n",
    "output = generated_text(input_text)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context is Everithing\n",
    "\n",
    "* **Context is Crucial:** The more context and details you provide in a prompt, the more accurate and reliable the model's response will be. Always consider what information the model might need to know to generate the best possible answer.\n",
    "\n",
    "* **Prompt Tailoring:** Adjust your prompts according to the specific scenario or task at hand. Whether you’re handling customer service queries, creating content, or seeking coding help, a well-crafted prompt will lead to better outcomes.\n",
    "\n",
    "Let's see if we can improve the answer using a better context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 390 tokens\n",
      "\n",
      "User: whome do i speak to?\n",
      "Assistent: You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\n",
      "User: What do you know about Entiros?\n",
      "Assistent: This is what i found on the wab page of Entiros: Better ways to build integrations. Data-informed connectivity. Our approach combines data analysis with experience to make data-informed network decisions, ensuring relevance and effectiveness in every integration.. Data-driven connectivity. Customer case. Find out how adopting technological advancements simplifies processes, boosts expansion, and gears up for upcoming obstacles in the constantly changing automotive sector.. \"Lorem Ipsum text asd asd asd asd asd asd asd asd asd asd asd asd asd asd asd \". Företag ABC. John Johnsson. \"Lorem Ipsum\". Företag ABC. John Johnson. Let's connect. About us. Entiros is a specialized integration company full of integrators, architects, and talented people dedicated to connecting the world.. We offer better ways to build integrations by addressing fundamental industry challenges - both for large companies and people in the industry. We support you through the entire chain, from integration strategy to implementation, management, and operations.. Other services we offer. Starlify. Build better integrations by collaboration and insight. Starlify is a productivity tool for creating smarter connectivity in any IT landscape.. Certified Integrator. Partnerships. Solutions. Event streams\n",
      "Our network. Our network. About us. Integration blog. Connect. An Entiros company. © 2024 Entiros. All rights reserved.\n",
      "User: Can you give me a short summary of what Entiros does?\n",
      "Assistent: 1. Better ways to build integrations. 2. Data-informed connectivity. 3. Our approach combines data analysis with experience to make data-informed network decisions, ensuring relevance and effectiveness in every integration.. Data-informed connectivity. Customer case. Find out how adopting technological advancements simplifies processes, boosts expansion, and gears up for upcoming obstacles in the constantly changing automotive sector.. \"Lorem Ipsum text asd asd asd asd asd asd asd asd as\n"
     ]
    }
   ],
   "source": [
    "import requests_html\n",
    "\n",
    "session = requests_html.HTMLSession()\n",
    "url = \"https://www.entiros.se/\"\n",
    "response = session.get(url)\n",
    "# get all the text from the p abd h tags\n",
    "web_info = response.html.xpath('//p | //h1 | //h2 | //h3 | //h4 | //h5 | //h6 | //li')\n",
    "web_info = [t.text for t in web_info]\n",
    "web_info = \". \".join(web_info)\n",
    "\n",
    "input_text = f\"\"\"\n",
    "User: whome do i speak to?\n",
    "Assistent: You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\n",
    "User: What do you know about Entiros?\n",
    "Assistent: This is what i found on the wab page of Entiros: {web_info}\n",
    "User: Can you give me a short summary of what Entiros does?\n",
    "Assistent: \"\"\"\n",
    "\n",
    "output = generated_text(input_text)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Tokens?\n",
    "In the context of Large Language Models (LLMs), tokens are the individual pieces of text that the model processes. A token can be as small as a single character, like a letter or punctuation mark, or as large as a full word or subword, depending on the specific tokenization method used by the model.\n",
    "\n",
    "* **Tokenization:** Before an LLM can process text, it needs to break the text down into tokens. For example, the sentence \"Hello, world!\" might be tokenized into [\"Hello\", \",\", \"world\", \"!\"]. The way text is split into tokens depends on the model and the tokenizer associated with it.\n",
    "Processing Text: The model then processes these tokens sequentially, using them to predict the next token or generate a response based on the sequence of tokens it has seen so far.\n",
    "What Is the Context Window?\n",
    "The context window refers to the maximum number of tokens that an LLM can process at one time. This is also known as the model's \"maximum sequence length.\" For instance, if an LLM has a context window of 512 tokens, it can only consider the most recent 512 tokens when generating its next output.\n",
    "\n",
    "* **Context Window Size:** The size of the context window is crucial because it determines how much information the model can keep track of at once. A larger context window allows the model to consider more context when generating responses, which can lead to more coherent and contextually relevant outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19556, 28, 10369, 89, 4066, 17]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a text\n",
    "ids = tokenizer.encode(\"Hello, Entiros!\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello -> 19556\n",
      ", -> 28\n",
      " Ent -> 10369\n",
      "i -> 89\n",
      "ros -> 4066\n",
      "! -> 17\n"
     ]
    }
   ],
   "source": [
    "# ids to text representation\n",
    "for id in ids:\n",
    "    print(f\"{tokenizer.decode(id)} -> {id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://blog.entiros.se/en/blog-media/from-data-lakes-to-dynamic-data-ecosystems\n",
      "https://blog.entiros.se/en/blog-media/the-challenges-of-point-to-point-connectivity-in-it-infrastructure\n",
      "https://blog.entiros.se/en/blog-media/the-importance-of-data-for-business-intelligence\n",
      "https://blog.entiros.se/en/blog-media/entiros-launches-starlify-in-swedish-cloud-service-approved-by-skr\n",
      "https://blog.entiros.se/en/blog-media/7-ways-integration-discovery-can-enhance-your-psd2-compliance\n",
      "https://blog.entiros.se/en/blog-media/choosing-the-right-api-technology\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The role of data lakes is undergoing a significant transformation. Traditionally viewed as mere repositories for storing vast amounts of data, the modern approach to utilizing data lakes is much more dynamic and strategic, particularly when enhancing Business Intelligence (BI) operations.\\nThe Traditional Data Lake: A Repository, Not a Source\\nHistorically, data lakes have been treated as data sinks — places where data is accumulated from various sources but seldom retrieved for real-time analysis. The common perception was that once data was stored in a data lake, it was not to be disturbed except for predefined reporting and BI tasks. This originated from concerns that the data, often batch-processed from dozens if not hundreds of applications, was too stale for real-time decision-making.\\nHowever, a paradigm shift is now reshaping how organizations approach their integration networks. Companies are moving towards real-time data flow systems rather than relying on traditional Extract, Transform, Load (ETL) methods, which often result in delays and outdated information. This shift means that data now travels from creation to data lake in seconds, significantly refreshing the quality and timeliness of the information stored.\\nUtilizing Data Lakes as Active Databases\\nWith this new approach, data lakes are transitioning from static data repositories to active, operational databases. This shift allows for implementing RESTful operations directly over the data lake, transforming it into a highly responsive and scalable data management system. Such capabilities enable organizations to store data and interact with it dynamically to drive real-time business intelligence and analytics.\\nImplementing Advanced Data Strategies\\nOrganizations can gather and compile data more effectively by accessing multiple data sources, including real-time streams from a data lake. The ability to tap into a centrally managed yet highly accessible data lake allows for more robust and scalable integration of data sources, facilitating more comprehensive and agile BI solutions.\\nThe Impact on Business Intelligence\\nFor BI teams, the implications of these advancements are profound. Instead of pulling outdated data for analysis, BI professionals can now access the most current data streams, enabling more accurate and timely insights. This capability enhances the quality of business decisions and supports more sophisticated analytics, such as predictive modeling and real-time analytics.\\nMoreover, integrating real-time data flows helps bridge the gap between different data domains within an organization, fostering better collaboration and data sharing among departments. As data flows become more integrated and responsive, the entire organization benefits from a more unified and agile approach to data management.\\nLooking Ahead\\nAs we look to the future, the role of data lakes\\xa0is set\\xa0to become even more central in enterprises’ data ecosystems. With advancements in technology and integration strategies, data lakes are poised to become not just storage facilities but vital components of an enterprise’s data architecture - powering innovation and operational efficiency.\\n\\n Explore the common challenges faced in IT infrastructure when using point-to-point connectivity and learn how to overcome them. How can Self-Service Integration solve many of the challenges?\\nPoint-to-Point Connectivity Limitations\\nWhile point-to-point connectivity offers direct communication between two endpoints, it has certain limitations that can impact IT infrastructure. One major limitation is managing multiple point-to-point connections\\' complexity and maintenance overhead. Monitoring and troubleshooting each connection individually becomes challenging as the number of connections increases.\\nAnother limitation is the lack of scalability. Point-to-point connections are usually established one-to-one, meaning adding new endpoints requires creating additional connections. This can lead to a tangled web of connections that becomes difficult to manage and scale as the infrastructure grows.\\nFurthermore, point-to-point connectivity can result in vendor lock-in. Each connection is typically configured with specific protocols and standards, making switching vendors or integrating new technologies challenging. This lack of interoperability can limit the organization\\'s ability to adopt new solutions or take advantage of emerging technologies.\\nLastly, point-to-point connections can also introduce security risks. Each connection represents a potential entry point for cyber threats, and securing each connection individually can be a complex task. Additionally, point-to-point connections may not provide the security and encryption required to protect sensitive data.\\nSecurity Risks and Vulnerabilities\\nPoint-to-point connectivity in IT infrastructure can introduce various security risks and vulnerabilities. One of the main concerns is the lack of centralized security controls. With multiple point-to-point connections, enforcing consistent security policies across the entire network becomes challenging.\\nAnother risk is the potential exposure of sensitive data during transit. Point-to-point connections may not provide adequate encryption or secure transmission protocols, leaving data vulnerable to interception or unauthorized access.\\nFurthermore, each point-to-point connection represents a potential entry point for cyber threats. If one connection is compromised, it can expose the entire network to security breaches. This makes it crucial to implement robust security measures at each endpoint and ensure regular monitoring and updates to prevent and detect any potential vulnerabilities.\\nTo mitigate these risks, organizations should consider implementing a comprehensive security strategy that includes strong encryption protocols, regular security audits, and network segmentation to minimize the impact of a potential breach.\\nScalability Issues in IT Infrastructure\\nPoint-to-point connectivity can pose scalability challenges in IT infrastructure. As connections increase, managing and scaling the infrastructure becomes more complex and time-consuming.\\nAdding new endpoints requires creating additional connections, resulting in a tangled web of connections that is difficult to manage and maintain. This can lead to increased operational costs and reduced efficiency.\\nMoreover, scaling point-to-point connections often requires manual configuration and intervention, which can be time-consuming and error-prone. This manual process can hinder the organization\\'s ability to quickly adapt to changing business requirements and integrate new applications or systems.\\nOrganizations can leverage self-service integration solutions that automate connectivity to address these scalability issues. Organizations can easily add or remove endpoints using self-service integration without manual intervention. This improves scalability, reduces the risk of errors, and accelerates the deployment of new connections.\\nImpact on Network Performance\\nPoint-to-point connectivity can have a significant impact on network performance. As the number of connections increases, the network can become congested, leading to latency and reduced overall performance.\\nEach point-to-point connection consumes network resources, including bandwidth and processing power. This can result in network bottlenecks and decreased performance for other applications and users.\\nFurthermore, managing and monitoring multiple point-to-point connections can be resource-intensive and require dedicated network administrators. This can divert resources from other critical tasks and impact overall network efficiency.\\nOrganizations should consider implementing alternative connectivity approaches, such as using a hub-and-spoke or mesh network topology to mitigate the impact on network performance. These approaches provide centralized management and better utilization of network resources, resulting in improved performance and scalability.\\nBenefits of Leveraging Self-Service Integration to Overcome P2P Obstacles\\nPoint-to-point connectivity in IT infrastructure can present several challenges, but leveraging self-service integration can help overcome these obstacles. By using self-service integration, organizations can streamline their connectivity processes and reduce the need for manual intervention. This saves time and resources and minimizes the risk of errors that can occur during manual configuration.\\nSelf-service integration also enables better collaboration between different systems and applications. It allows seamless data exchange and synchronization, facilitating efficient communication between various IT infrastructure components. This can improve overall productivity and enhance the user experience.\\nAdditionally, self-service integration provides greater flexibility and agility in adapting to changing business requirements. It allows organizations to quickly integrate new applications or systems into their infrastructure without disrupting the entire network. This scalability and adaptability can greatly benefit organizations in today\\'s fast-paced and dynamic business environment.\\n\\n Today, the Integration Center is working very well in describing its application networks and building integrations, and it has many different parties that need data. However, one department in the company needs data more than anyone else - the BI department.\\nThe Business Intelligence department might sit with a data lake and a BI function. Many are still working with analytical BI, where one might look at the last week\\'s data, the last month\\'s data, and, in the best case, the last day\\'s data. Very few are working with operational BI yet, meaning they don\\'t understand what is happening in the reports. How do we act in our business right now to meet certain needs?\\nThe integration centers play an extremely important role here. The BI team needs more data than any other team, and they need current data. Therefore, it is necessary to supplement batch data loading with streaming data. New solutions are needed, for example, based on Kafka instead of using ETL tools.\\nWhat is particularly interesting now is that the BI team is not only working with business intelligence; they have started working with artificial intelligence, or AI. An extremely important aspect of this is the availability of data. You can only train a good model if you have data to train on. The companies that have not yet started collecting data in their data lakes will not have any data to train on. If we see the BI development continue as quickly as it is now, including the specific area of AI, there will be a watershed within maybe a year. The companies that have data to apply their AI on and those that do NOT have data to apply their AI on. And this will then be decided by not how good a BI product you have but how good an integration product you have. How much data have you transported from your application network to your BI team?\\nWhat is required of the BI team for this to happen?\\nThey need to understand how the company\\'s application network looks.\\nWhere is data located?\\nWhat data are we transporting today?\\nWhat data SHOULD we be transporting today to be able to do better BI and even AI tomorrow?\\nBI and integration managers need to talk more to each other. This is about today\\'s challenge of achieving operational BI and tomorrow\\'s challenge of getting data to train on for the coming AI function.\\nOtherwise, there will be no cognitive systems in the company.\\nNo data - no cognition.\\n\\n In a groundbreaking move, Entiros unveiled Starlify, a cutting-edge solution designed to address public sector organizations\\' pervasive challenges in managing their system landscapes. The lack of clarity and understanding of the current state often hinders effective decision-making, leading to increased costs and inefficiencies in IT projects.\\nStarlify Launches in Swedish Cloud Service Endorsed by SKR\\nEntiros proudly announces the official launch of Starlify in a Swedish cloud service approved by SKR (Swedish Association of Local Authorities and Regions). This strategic move positions Starlify as a secure and reliable solution for public sector entities in Sweden.\\nRevolutionizing System Overview and Efficiency\\nPublic sector organizations have started using the tool to find dependencies and improve efficiency. From before, among others, Volvo Cars, with one of Sweden\\'s largest application networks, uses Starlify. Entiros is now looking forward to further collaborations to support Sweden\\'s municipalities and regions in their challenges.\\nAddressing Critical Issues in Public Sector IT\\nThe struggle to understand the system\\'s current state in the public sector often results in ineffective strategies, resource wastage, and budget breaches. In addition, it becomes difficult to see and follow the benefits and effects of the projects. As an example, over SEK 3 billion (2018), out of a total of SEK 9.4 billion, went to projects where the benefit or effect could not be followed after the end of the project. This ultimately impacts the ability to deliver high-quality services to the community. Starlify aims to rectify this by providing a user-friendly tool to collect and create comprehensive overviews of systems and applications.\\nKey Features and User Benefits\\nStarlify facilitates quick mapping and visualization, enabling efficient planning before implementing changes. This occurs within a secure environment that fosters collaboration between internal and external suppliers. Crucially, the tool supports the creation of efficient management processes for sensitive information, ensuring compliance with regulations.\\nAnders Holmstrand, Product Manager at Starlify, Comments:\\n\"Starlify empowers public sector organizations to take control of their system landscape and effectively manage changes with insight and security. Our presence at the Solutions for Public Sector trade show in January offers an excellent opportunity for visitors to learn more about our tool and discuss their challenges in IT projects and change work.\"\\n\\n What is PSD2?\\nPSD2, short for the Second Payment Services Directive, aims to revolutionize payment services. This regulation opens consumer bank accounts to third-party providers (TPPs). Unlocking vast amounts of data banks hold. It levels the playing field and bridges the gap between traditional banks, fintech, and big techs. It represents a shift in the European banking sector, moving it toward the era of Open Banking.\\nHow it Affects Banks\\nThe impact of PSD2 on banks is monumental. This regulation grants third-party providers access to bank data. However, its implementation faced hurdles, experiencing delays and partial enforcement in different countries. Two of the core aspects of PSD2 faced challenges in full deployment. These were strong customer authentication (SCA) and standardized APIs for account interfaces. The banks navigated through these complexities while trying to adapt to the regulatory landscape.\\nWhy Must Banks Adapt Integration Landscapes for PSD2 Compliance?\\nThe challenge for banks is twofold: achieving compliance while maintaining technological readiness. This requires meeting defined standards and preparing for potential revisions and expansions. Banks must balance regulatory compliance demands with the agility needed to adapt. The adaptation process is a continuous journey that\\'s necessary to secure compliance. This ongoing adjustment ensures that banks are ready for upcoming regulatory changes.\\nIntegration Discovery for PSD2 Compliance\\nImplementing an integration solution for PSD2 compliance requires a strategic approach. An approach that guarantees efficiency and rapid adaptability. Banks and other financial institutions must chart a straightforward roadmap to navigate both swiftly and effectively. Otherwise, it\\'s unlikely that the PSD2 compliance will succeed. How do they do it? Integration discovery.\\n1. Strategic Alignment and Objectives Setting:\\nPrioritize aligning your integration strategy with the overarching goals of PSD2 compliance. Set clear objectives and outline the wanted outcomes from the integration process. Establish a dedicated team with IT, compliance, security, and business strategy experts to work with the initiative.\\n2. Comprehensive Landscape Assessment:\\nInitiate with a complete assessment of your current integration landscape. Engage stakeholders, IT teams, and relevant departments to understand your current state. That involves existing systems, APIs, data architecture, security protocols, and integrations. Document and analyze the current state and potential areas for improvement. HÄR KAN STARLIFY HJÄLPA TILL.\\n3. Technology Stack Evaluation:\\nEvaluate your technology stack against PSD2 compliance requirements. Identify any gaps between the existing technology stack and the compliance prerequisites.\\n4. Collaboration and Partnership Exploration:\\nFoster collaborations and partnerships within the industry and engage with both third-party providers and internal experts to leverage their expertise and experiences. Explore collaborations that can expedite the implementation of necessary changes and adaptations to meet compliance requirements. And, of course, collaborations that can help you get an overview of your current state.\\n5. Creation of a Rapid Deployment Plan:\\nUtilize the insights from the assessment and analysis phases to devise a rapid deployment plan. Prioritize actions that address critical gaps and compliance necessities. Craft a phased approach, allowing incremental improvements while aligning with regulatory timelines.\\n6. Implementation and Continuous Iteration:\\nImplement the planned changes and adaptations in a phased manner. Continuously refine the integration discovery process based on real-time insights and developments. Review progress and adapt the approach to ensure swift compliance without compromising quality.\\n7. Documentation and Knowledge Transfer:\\nDocument every step, change, and implementation made throughout the integration discovery process. Make sure your organization and relevant stakeholders understand the insights gained and the solutions implemented. This can be achieved with a solution that visualizes and shows dependencies across your application network.\\n\\n Are you tired of hearing about REST, GraphQL, and Asynch APIs without really understanding what they mean? Well, don\\'t worry; you\\'re not alone. In the world of application development, there are a lot of different technologies to choose from, and it can be overwhelming to figure out which one is right for your project. So let\\'s break it down.\\nRESTful APIs; Best for simple CRUD operations\\nFirst up, we have REST. Now, I know what you\\'re thinking, \"Representational State Transfer, that sounds fancy and confusing!\" But it\\'s really not that complicated. RESTful APIs use HTTP requests to POST (create), PUT (update), GET (read), and DELETE data, and they\\'re based on familiar concepts like URLs and HTTP. They are also often used with web services that return data in JSON or XML format.\\nGraphQL; Best for complex data requirements\\nNext, we have GraphQL, which is a newer technology developed by Facebook. GraphQL is a query language that allows developers to request exactly the data they need rather than getting a fixed set of data from an endpoint. It\\'s kind of like going to a restaurant and being able to order exactly what you want rather than being forced to order a pre-set meal. This can make it more efficient to work with, especially for more complex data requirements. In addition, GraphQL also provides a type system that allows for more efficient validation and better documentation of the API.\\nAsynch APIs; Best for real-time applications\\nLast but not least, we have Asynch APIs, which are becoming increasingly popular recently. As the name suggests, these APIs work asynchronously, which means that clients can make requests and receive responses as soon as they\\'re available rather than having to wait for a response from the server. This can be really useful for real-time applications like chat or gaming.\\nChoosing the API technology that best fits your needs\\nSo which one is right for your project? Well, it depends on your specific needs. If you\\'re working on a simple CRUD (create, read, update, delete) application, RESTful APIs are probably a good choice. If you need more complex data requirements, GraphQL might be a better fit. And if you\\'re working on a real-time application, Asynch APIs are definitely worth considering.\\nSeek guidance from an integration company if you need help\\nAt the end of the day, the most important thing is to evaluate the requirements of your project and choose the technology that best fits your needs. And if you\\'re still feeling overwhelmed, don\\'t hesitate to reach out to an integration company (like Entiros Integrations 😎) for guidance. They\\'ll be able to help you navigate the world of APIs and choose the right one for your project.'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://blog.entiros.se/en/blog-media\"\n",
    "response = session.get(url)\n",
    "blogs = response.html.xpath(r\"//a[contains(@class, 'blog-post__post')]\")\n",
    "texts = []\n",
    "for item in blogs:\n",
    "    print(item.attrs['href'])\n",
    "    response = session.get(item.attrs['href'])\n",
    "    blog_text = response.html.xpath(r\"//article\")\n",
    "    texts.append(blog_text[0].text)\n",
    "\n",
    "policy_info = \"\\n\\n \".join(texts)\n",
    "policy_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 3844 tokens\n"
     ]
    }
   ],
   "source": [
    "encoded_wiki_info = tokenizer.encode(policy_info, return_tensors=\"pt\").to(device)\n",
    "print(f\"Maximum sequence length: {config.max_position_embeddings} tokens, input is {encoded_wiki_info.shape[1]} tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what can we do when the input is too long?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabby\\workshop_llms\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embed_tokenizer = AutoTokenizer.from_pretrained(embed_model_name)\n",
    "embed_model = AutoModel.from_pretrained(embed_model_name)\n",
    "embed_config = AutoConfig.from_pretrained(embed_model_name)\n",
    "max_position_embeddings = embed_config.max_position_embeddings\n",
    "\n",
    "def embed_text(text, token_length=max_position_embeddings, overlap=max_position_embeddings//2):\n",
    "    # Tokenize the input text\n",
    "    inputs = embed_tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    print(f\"Maximum sequence length: {max_position_embeddings} tokens, input is {inputs['input_ids'].shape[1]} tokens\")\n",
    "\n",
    "    # Split the input text into smaller parts\n",
    "    input_list = inputs[\"input_ids\"].reshape(-1).tolist()\n",
    "    cls = embed_tokenizer.cls_token_id\n",
    "    n = len(input_list)\n",
    "\n",
    "    input_ids = [ [cls] + input_list[i:i+token_length-1] for i in range(0, n-token_length-1, token_length-overlap-1)] # this will drop the last part \n",
    "    input_ids.append([cls] + input_list[-token_length+1:]) # add the last part\n",
    "    lookup = [embed_tokenizer.decode(ids[1:]) for ids in input_ids]\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "\n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        embeddings = embed_model(input_ids).last_hidden_state[:, 0, :]\n",
    "\n",
    "    # Convert embeddings to numpy arrays\n",
    "    return embeddings.numpy(), lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3868 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 512 tokens, input is 3868 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30, 384)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectors, lookup = embed_text(policy_info, token_length=256, overlap=128)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the role of data lakes is undergoing a significant transformation. traditionally viewed as mere repositories for storing vast amounts of data, the modern approach to utilizing data lakes is much more dynamic and strategic, particularly when enhancing business intelligence ( bi ) operations. the traditional data lake : a repository, not a source historically, data lakes have been treated as data sinks — places where data is accumulated from various sources but seldom retrieved for real - time analysis. the common perception was that once data was stored in a data lake, it was not to be disturbed except for predefined reporting and bi tasks. this originated from concerns that the data, often batch - processed from dozens if not hundreds of applications, was too stale for real - time decision - making. however, a paradigm shift is now reshaping how organizations approach their integration networks. companies are moving towards real - time data flow systems rather than relying on traditional extract, transform, load ( etl ) methods, which often result in delays and outdated information. this shift means that data now travels from creation to data lake in seconds, significantly refreshing the quality and timeliness of the information stored. utilizing data lakes as active databases with this new approach, data lakes are transitioning from static data repositories to active',\n",
       " ', often batch - processed from dozens if not hundreds of applications, was too stale for real - time decision - making. however, a paradigm shift is now reshaping how organizations approach their integration networks. companies are moving towards real - time data flow systems rather than relying on traditional extract, transform, load ( etl ) methods, which often result in delays and outdated information. this shift means that data now travels from creation to data lake in seconds, significantly refreshing the quality and timeliness of the information stored. utilizing data lakes as active databases with this new approach, data lakes are transitioning from static data repositories to active, operational databases. this shift allows for implementing restful operations directly over the data lake, transforming it into a highly responsive and scalable data management system. such capabilities enable organizations to store data and interact with it dynamically to drive real - time business intelligence and analytics. implementing advanced data strategies organizations can gather and compile data more effectively by accessing multiple data sources, including real - time streams from a data lake. the ability to tap into a centrally managed yet highly accessible data lake allows for more robust and scalable integration of data sources, facilitating more comprehensive and agile bi solutions. the impact on business intelligence for bi teams, the',\n",
       " 'active, operational databases. this shift allows for implementing restful operations directly over the data lake, transforming it into a highly responsive and scalable data management system. such capabilities enable organizations to store data and interact with it dynamically to drive real - time business intelligence and analytics. implementing advanced data strategies organizations can gather and compile data more effectively by accessing multiple data sources, including real - time streams from a data lake. the ability to tap into a centrally managed yet highly accessible data lake allows for more robust and scalable integration of data sources, facilitating more comprehensive and agile bi solutions. the impact on business intelligence for bi teams, the implications of these advancements are profound. instead of pulling outdated data for analysis, bi professionals can now access the most current data streams, enabling more accurate and timely insights. this capability enhances the quality of business decisions and supports more sophisticated analytics, such as predictive modeling and real - time analytics. moreover, integrating real - time data flows helps bridge the gap between different data domains within an organization, fostering better collaboration and data sharing among departments. as data flows become more integrated and responsive, the entire organization benefits from a more unified and agile approach to data management. looking ahead as we look to the future, the role of data',\n",
       " 'the implications of these advancements are profound. instead of pulling outdated data for analysis, bi professionals can now access the most current data streams, enabling more accurate and timely insights. this capability enhances the quality of business decisions and supports more sophisticated analytics, such as predictive modeling and real - time analytics. moreover, integrating real - time data flows helps bridge the gap between different data domains within an organization, fostering better collaboration and data sharing among departments. as data flows become more integrated and responsive, the entire organization benefits from a more unified and agile approach to data management. looking ahead as we look to the future, the role of data lakes is set to become even more central in enterprises ’ data ecosystems. with advancements in technology and integration strategies, data lakes are poised to become not just storage facilities but vital components of an enterprise ’ s data architecture - powering innovation and operational efficiency. explore the common challenges faced in it infrastructure when using point - to - point connectivity and learn how to overcome them. how can self - service integration solve many of the challenges? point - to - point connectivity limitations while point - to - point connectivity offers direct communication between two endpoints, it has certain limitations that can impact it infrastructure. one major limitation is managing multiple point - to',\n",
       " \"data lakes is set to become even more central in enterprises ’ data ecosystems. with advancements in technology and integration strategies, data lakes are poised to become not just storage facilities but vital components of an enterprise ’ s data architecture - powering innovation and operational efficiency. explore the common challenges faced in it infrastructure when using point - to - point connectivity and learn how to overcome them. how can self - service integration solve many of the challenges? point - to - point connectivity limitations while point - to - point connectivity offers direct communication between two endpoints, it has certain limitations that can impact it infrastructure. one major limitation is managing multiple point - to - point connections'complexity and maintenance overhead. monitoring and troubleshooting each connection individually becomes challenging as the number of connections increases. another limitation is the lack of scalability. point - to - point connections are usually established one - to - one, meaning adding new endpoints requires creating additional connections. this can lead to a tangled web of connections that becomes difficult to manage and scale as the infrastructure grows. furthermore, point - to - point connectivity can result in vendor lock - in. each connection is typically configured with specific protocols and standards, making switching vendors or integrating new technologies challenging. this lack of interoperability can limit the organization\",\n",
       " \"to - point connections'complexity and maintenance overhead. monitoring and troubleshooting each connection individually becomes challenging as the number of connections increases. another limitation is the lack of scalability. point - to - point connections are usually established one - to - one, meaning adding new endpoints requires creating additional connections. this can lead to a tangled web of connections that becomes difficult to manage and scale as the infrastructure grows. furthermore, point - to - point connectivity can result in vendor lock - in. each connection is typically configured with specific protocols and standards, making switching vendors or integrating new technologies challenging. this lack of interoperability can limit the organization's ability to adopt new solutions or take advantage of emerging technologies. lastly, point - to - point connections can also introduce security risks. each connection represents a potential entry point for cyber threats, and securing each connection individually can be a complex task. additionally, point - to - point connections may not provide the security and encryption required to protect sensitive data. security risks and vulnerabilities point - to - point connectivity in it infrastructure can introduce various security risks and vulnerabilities. one of the main concerns is the lack of centralized security controls. with multiple point - to - point connections, enforcing consistent security policies across the entire\",\n",
       " \"organization's ability to adopt new solutions or take advantage of emerging technologies. lastly, point - to - point connections can also introduce security risks. each connection represents a potential entry point for cyber threats, and securing each connection individually can be a complex task. additionally, point - to - point connections may not provide the security and encryption required to protect sensitive data. security risks and vulnerabilities point - to - point connectivity in it infrastructure can introduce various security risks and vulnerabilities. one of the main concerns is the lack of centralized security controls. with multiple point - to - point connections, enforcing consistent security policies across the entire network becomes challenging. another risk is the potential exposure of sensitive data during transit. point - to - point connections may not provide adequate encryption or secure transmission protocols, leaving data vulnerable to interception or unauthorized access. furthermore, each point - to - point connection represents a potential entry point for cyber threats. if one connection is compromised, it can expose the entire network to security breaches. this makes it crucial to implement robust security measures at each endpoint and ensure regular monitoring and updates to prevent and detect any potential vulnerabilities. to mitigate these risks, organizations should consider implementing a comprehensive security strategy that includes strong encryption protocols, regular\",\n",
       " \"entire network becomes challenging. another risk is the potential exposure of sensitive data during transit. point - to - point connections may not provide adequate encryption or secure transmission protocols, leaving data vulnerable to interception or unauthorized access. furthermore, each point - to - point connection represents a potential entry point for cyber threats. if one connection is compromised, it can expose the entire network to security breaches. this makes it crucial to implement robust security measures at each endpoint and ensure regular monitoring and updates to prevent and detect any potential vulnerabilities. to mitigate these risks, organizations should consider implementing a comprehensive security strategy that includes strong encryption protocols, regular security audits, and network segmentation to minimize the impact of a potential breach. scalability issues in it infrastructure point - to - point connectivity can pose scalability challenges in it infrastructure. as connections increase, managing and scaling the infrastructure becomes more complex and time - consuming. adding new endpoints requires creating additional connections, resulting in a tangled web of connections that is difficult to manage and maintain. this can lead to increased operational costs and reduced efficiency. moreover, scaling point - to - point connections often requires manual configuration and intervention, which can be time - consuming and error - prone. this manual process can hinder the organization's\",\n",
       " \"regular security audits, and network segmentation to minimize the impact of a potential breach. scalability issues in it infrastructure point - to - point connectivity can pose scalability challenges in it infrastructure. as connections increase, managing and scaling the infrastructure becomes more complex and time - consuming. adding new endpoints requires creating additional connections, resulting in a tangled web of connections that is difficult to manage and maintain. this can lead to increased operational costs and reduced efficiency. moreover, scaling point - to - point connections often requires manual configuration and intervention, which can be time - consuming and error - prone. this manual process can hinder the organization's ability to quickly adapt to changing business requirements and integrate new applications or systems. organizations can leverage self - service integration solutions that automate connectivity to address these scalability issues. organizations can easily add or remove endpoints using self - service integration without manual intervention. this improves scalability, reduces the risk of errors, and accelerates the deployment of new connections. impact on network performance point - to - point connectivity can have a significant impact on network performance. as the number of connections increases, the network can become congested, leading to latency and reduced overall performance. each point - to - point connection consumes network resources, including bandwidth\",\n",
       " 's ability to quickly adapt to changing business requirements and integrate new applications or systems. organizations can leverage self - service integration solutions that automate connectivity to address these scalability issues. organizations can easily add or remove endpoints using self - service integration without manual intervention. this improves scalability, reduces the risk of errors, and accelerates the deployment of new connections. impact on network performance point - to - point connectivity can have a significant impact on network performance. as the number of connections increases, the network can become congested, leading to latency and reduced overall performance. each point - to - point connection consumes network resources, including bandwidth and processing power. this can result in network bottlenecks and decreased performance for other applications and users. furthermore, managing and monitoring multiple point - to - point connections can be resource - intensive and require dedicated network administrators. this can divert resources from other critical tasks and impact overall network efficiency. organizations should consider implementing alternative connectivity approaches, such as using a hub - and - spoke or mesh network topology to mitigate the impact on network performance. these approaches provide centralized management and better utilization of network resources, resulting in improved performance and scalability. benefits of leveraging self - service integration to overcome p2p obstacles point - to - point',\n",
       " 'bandwidth and processing power. this can result in network bottlenecks and decreased performance for other applications and users. furthermore, managing and monitoring multiple point - to - point connections can be resource - intensive and require dedicated network administrators. this can divert resources from other critical tasks and impact overall network efficiency. organizations should consider implementing alternative connectivity approaches, such as using a hub - and - spoke or mesh network topology to mitigate the impact on network performance. these approaches provide centralized management and better utilization of network resources, resulting in improved performance and scalability. benefits of leveraging self - service integration to overcome p2p obstacles point - to - point connectivity in it infrastructure can present several challenges, but leveraging self - service integration can help overcome these obstacles. by using self - service integration, organizations can streamline their connectivity processes and reduce the need for manual intervention. this saves time and resources and minimizes the risk of errors that can occur during manual configuration. self - service integration also enables better collaboration between different systems and applications. it allows seamless data exchange and synchronization, facilitating efficient communication between various it infrastructure components. this can improve overall productivity and enhance the user experience. additionally, self - service integration provides greater flexibility and agility in adapting to changing business requirements',\n",
       " \"point connectivity in it infrastructure can present several challenges, but leveraging self - service integration can help overcome these obstacles. by using self - service integration, organizations can streamline their connectivity processes and reduce the need for manual intervention. this saves time and resources and minimizes the risk of errors that can occur during manual configuration. self - service integration also enables better collaboration between different systems and applications. it allows seamless data exchange and synchronization, facilitating efficient communication between various it infrastructure components. this can improve overall productivity and enhance the user experience. additionally, self - service integration provides greater flexibility and agility in adapting to changing business requirements. it allows organizations to quickly integrate new applications or systems into their infrastructure without disrupting the entire network. this scalability and adaptability can greatly benefit organizations in today's fast - paced and dynamic business environment. today, the integration center is working very well in describing its application networks and building integrations, and it has many different parties that need data. however, one department in the company needs data more than anyone else - the bi department. the business intelligence department might sit with a data lake and a bi function. many are still working with analytical bi, where one might look at the last week's data, the last\",\n",
       " \"requirements. it allows organizations to quickly integrate new applications or systems into their infrastructure without disrupting the entire network. this scalability and adaptability can greatly benefit organizations in today's fast - paced and dynamic business environment. today, the integration center is working very well in describing its application networks and building integrations, and it has many different parties that need data. however, one department in the company needs data more than anyone else - the bi department. the business intelligence department might sit with a data lake and a bi function. many are still working with analytical bi, where one might look at the last week's data, the last month's data, and, in the best case, the last day's data. very few are working with operational bi yet, meaning they don't understand what is happening in the reports. how do we act in our business right now to meet certain needs? the integration centers play an extremely important role here. the bi team needs more data than any other team, and they need current data. therefore, it is necessary to supplement batch data loading with streaming data. new solutions are needed, for example, based on kafka instead of using etl tools. what is particularly interesting now is that the bi team is\",\n",
       " \"last month's data, and, in the best case, the last day's data. very few are working with operational bi yet, meaning they don't understand what is happening in the reports. how do we act in our business right now to meet certain needs? the integration centers play an extremely important role here. the bi team needs more data than any other team, and they need current data. therefore, it is necessary to supplement batch data loading with streaming data. new solutions are needed, for example, based on kafka instead of using etl tools. what is particularly interesting now is that the bi team is not only working with business intelligence ; they have started working with artificial intelligence, or ai. an extremely important aspect of this is the availability of data. you can only train a good model if you have data to train on. the companies that have not yet started collecting data in their data lakes will not have any data to train on. if we see the bi development continue as quickly as it is now, including the specific area of ai, there will be a watershed within maybe a year. the companies that have data to apply their ai on and those that do not have data to apply their ai on. and this will then be decided\",\n",
       " \"is not only working with business intelligence ; they have started working with artificial intelligence, or ai. an extremely important aspect of this is the availability of data. you can only train a good model if you have data to train on. the companies that have not yet started collecting data in their data lakes will not have any data to train on. if we see the bi development continue as quickly as it is now, including the specific area of ai, there will be a watershed within maybe a year. the companies that have data to apply their ai on and those that do not have data to apply their ai on. and this will then be decided by not how good a bi product you have but how good an integration product you have. how much data have you transported from your application network to your bi team? what is required of the bi team for this to happen? they need to understand how the company's application network looks. where is data located? what data are we transporting today? what data should we be transporting today to be able to do better bi and even ai tomorrow? bi and integration managers need to talk more to each other. this is about today's challenge of achieving operational bi and tomorrow's challenge of getting data to train on for the coming ai\",\n",
       " \"decided by not how good a bi product you have but how good an integration product you have. how much data have you transported from your application network to your bi team? what is required of the bi team for this to happen? they need to understand how the company's application network looks. where is data located? what data are we transporting today? what data should we be transporting today to be able to do better bi and even ai tomorrow? bi and integration managers need to talk more to each other. this is about today's challenge of achieving operational bi and tomorrow's challenge of getting data to train on for the coming ai function. otherwise, there will be no cognitive systems in the company. no data - no cognition. in a groundbreaking move, entiros unveiled starlify, a cutting - edge solution designed to address public sector organizations'pervasive challenges in managing their system landscapes. the lack of clarity and understanding of the current state often hinders effective decision - making, leading to increased costs and inefficiencies in it projects. starlify launches in swedish cloud service endorsed by skr entiros proudly announces the official launch of starlify in a swedish cloud service approved by skr ( swedish association of local authorities and\",\n",
       " \"ai function. otherwise, there will be no cognitive systems in the company. no data - no cognition. in a groundbreaking move, entiros unveiled starlify, a cutting - edge solution designed to address public sector organizations'pervasive challenges in managing their system landscapes. the lack of clarity and understanding of the current state often hinders effective decision - making, leading to increased costs and inefficiencies in it projects. starlify launches in swedish cloud service endorsed by skr entiros proudly announces the official launch of starlify in a swedish cloud service approved by skr ( swedish association of local authorities and regions ). this strategic move positions starlify as a secure and reliable solution for public sector entities in sweden. revolutionizing system overview and efficiency public sector organizations have started using the tool to find dependencies and improve efficiency. from before, among others, volvo cars, with one of sweden's largest application networks, uses starlify. entiros is now looking forward to further collaborations to support sweden's municipalities and regions in their challenges. addressing critical issues in public sector it the struggle to understand the system's current state in the public sector often results in ineffective strategies, resource wastage, and budget breaches.\",\n",
       " \"and regions ). this strategic move positions starlify as a secure and reliable solution for public sector entities in sweden. revolutionizing system overview and efficiency public sector organizations have started using the tool to find dependencies and improve efficiency. from before, among others, volvo cars, with one of sweden's largest application networks, uses starlify. entiros is now looking forward to further collaborations to support sweden's municipalities and regions in their challenges. addressing critical issues in public sector it the struggle to understand the system's current state in the public sector often results in ineffective strategies, resource wastage, and budget breaches. in addition, it becomes difficult to see and follow the benefits and effects of the projects. as an example, over sek 3 billion ( 2018 ), out of a total of sek 9. 4 billion, went to projects where the benefit or effect could not be followed after the end of the project. this ultimately impacts the ability to deliver high - quality services to the community. starlify aims to rectify this by providing a user - friendly tool to collect and create comprehensive overviews of systems and applications. key features and user benefits starlify facilitates quick mapping and visualization, enabling efficient planning before implementing changes. this\",\n",
       " '. in addition, it becomes difficult to see and follow the benefits and effects of the projects. as an example, over sek 3 billion ( 2018 ), out of a total of sek 9. 4 billion, went to projects where the benefit or effect could not be followed after the end of the project. this ultimately impacts the ability to deliver high - quality services to the community. starlify aims to rectify this by providing a user - friendly tool to collect and create comprehensive overviews of systems and applications. key features and user benefits starlify facilitates quick mapping and visualization, enabling efficient planning before implementing changes. this occurs within a secure environment that fosters collaboration between internal and external suppliers. crucially, the tool supports the creation of efficient management processes for sensitive information, ensuring compliance with regulations. anders holmstrand, product manager at starlify, comments : \" starlify empowers public sector organizations to take control of their system landscape and effectively manage changes with insight and security. our presence at the solutions for public sector trade show in january offers an excellent opportunity for visitors to learn more about our tool and discuss their challenges in it projects and change work. \" what is psd2? psd2, short for the second payment',\n",
       " 'this occurs within a secure environment that fosters collaboration between internal and external suppliers. crucially, the tool supports the creation of efficient management processes for sensitive information, ensuring compliance with regulations. anders holmstrand, product manager at starlify, comments : \" starlify empowers public sector organizations to take control of their system landscape and effectively manage changes with insight and security. our presence at the solutions for public sector trade show in january offers an excellent opportunity for visitors to learn more about our tool and discuss their challenges in it projects and change work. \" what is psd2? psd2, short for the second payment services directive, aims to revolutionize payment services. this regulation opens consumer bank accounts to third - party providers ( tpps ). unlocking vast amounts of data banks hold. it levels the playing field and bridges the gap between traditional banks, fintech, and big techs. it represents a shift in the european banking sector, moving it toward the era of open banking. how it affects banks the impact of psd2 on banks is monumental. this regulation grants third - party providers access to bank data. however, its implementation faced hurdles, experiencing delays and partial enforcement in different countries. two of the core aspects of psd2',\n",
       " \"payment services directive, aims to revolutionize payment services. this regulation opens consumer bank accounts to third - party providers ( tpps ). unlocking vast amounts of data banks hold. it levels the playing field and bridges the gap between traditional banks, fintech, and big techs. it represents a shift in the european banking sector, moving it toward the era of open banking. how it affects banks the impact of psd2 on banks is monumental. this regulation grants third - party providers access to bank data. however, its implementation faced hurdles, experiencing delays and partial enforcement in different countries. two of the core aspects of psd2 faced challenges in full deployment. these were strong customer authentication ( sca ) and standardized apis for account interfaces. the banks navigated through these complexities while trying to adapt to the regulatory landscape. why must banks adapt integration landscapes for psd2 compliance? the challenge for banks is twofold : achieving compliance while maintaining technological readiness. this requires meeting defined standards and preparing for potential revisions and expansions. banks must balance regulatory compliance demands with the agility needed to adapt. the adaptation process is a continuous journey that's necessary to secure compliance. this ongoing adjustment ensures that banks are ready for upcoming regulatory changes. integration discovery for ps\",\n",
       " \"##2 faced challenges in full deployment. these were strong customer authentication ( sca ) and standardized apis for account interfaces. the banks navigated through these complexities while trying to adapt to the regulatory landscape. why must banks adapt integration landscapes for psd2 compliance? the challenge for banks is twofold : achieving compliance while maintaining technological readiness. this requires meeting defined standards and preparing for potential revisions and expansions. banks must balance regulatory compliance demands with the agility needed to adapt. the adaptation process is a continuous journey that's necessary to secure compliance. this ongoing adjustment ensures that banks are ready for upcoming regulatory changes. integration discovery for psd2 compliance implementing an integration solution for psd2 compliance requires a strategic approach. an approach that guarantees efficiency and rapid adaptability. banks and other financial institutions must chart a straightforward roadmap to navigate both swiftly and effectively. otherwise, it's unlikely that the psd2 compliance will succeed. how do they do it? integration discovery. 1. strategic alignment and objectives setting : prioritize aligning your integration strategy with the overarching goals of psd2 compliance. set clear objectives and outline the wanted outcomes from the integration process. establish a dedicated team with it, compliance, security, and business strategy experts to\",\n",
       " \"psd2 compliance implementing an integration solution for psd2 compliance requires a strategic approach. an approach that guarantees efficiency and rapid adaptability. banks and other financial institutions must chart a straightforward roadmap to navigate both swiftly and effectively. otherwise, it's unlikely that the psd2 compliance will succeed. how do they do it? integration discovery. 1. strategic alignment and objectives setting : prioritize aligning your integration strategy with the overarching goals of psd2 compliance. set clear objectives and outline the wanted outcomes from the integration process. establish a dedicated team with it, compliance, security, and business strategy experts to work with the initiative. 2. comprehensive landscape assessment : initiate with a complete assessment of your current integration landscape. engage stakeholders, it teams, and relevant departments to understand your current state. that involves existing systems, apis, data architecture, security protocols, and integrations. document and analyze the current state and potential areas for improvement. har kan starlify hjalpa till. 3. technology stack evaluation : evaluate your technology stack against psd2 compliance requirements. identify any gaps between the existing technology stack and the compliance prerequisites. 4. collaboration and partnership exploration : foster collaborations and partnerships within the industry and engage\",\n",
       " 'to work with the initiative. 2. comprehensive landscape assessment : initiate with a complete assessment of your current integration landscape. engage stakeholders, it teams, and relevant departments to understand your current state. that involves existing systems, apis, data architecture, security protocols, and integrations. document and analyze the current state and potential areas for improvement. har kan starlify hjalpa till. 3. technology stack evaluation : evaluate your technology stack against psd2 compliance requirements. identify any gaps between the existing technology stack and the compliance prerequisites. 4. collaboration and partnership exploration : foster collaborations and partnerships within the industry and engage with both third - party providers and internal experts to leverage their expertise and experiences. explore collaborations that can expedite the implementation of necessary changes and adaptations to meet compliance requirements. and, of course, collaborations that can help you get an overview of your current state. 5. creation of a rapid deployment plan : utilize the insights from the assessment and analysis phases to devise a rapid deployment plan. prioritize actions that address critical gaps and compliance necessities. craft a phased approach, allowing incremental improvements while aligning with regulatory timelines. 6. implementation and continuous iteration : implement the planned changes and adaptations in a phased',\n",
       " \"engage with both third - party providers and internal experts to leverage their expertise and experiences. explore collaborations that can expedite the implementation of necessary changes and adaptations to meet compliance requirements. and, of course, collaborations that can help you get an overview of your current state. 5. creation of a rapid deployment plan : utilize the insights from the assessment and analysis phases to devise a rapid deployment plan. prioritize actions that address critical gaps and compliance necessities. craft a phased approach, allowing incremental improvements while aligning with regulatory timelines. 6. implementation and continuous iteration : implement the planned changes and adaptations in a phased manner. continuously refine the integration discovery process based on real - time insights and developments. review progress and adapt the approach to ensure swift compliance without compromising quality. 7. documentation and knowledge transfer : document every step, change, and implementation made throughout the integration discovery process. make sure your organization and relevant stakeholders understand the insights gained and the solutions implemented. this can be achieved with a solution that visualizes and shows dependencies across your application network. are you tired of hearing about rest, graphql, and asynch apis without really understanding what they mean? well, don't worry ; you're not alone\",\n",
       " 'phased manner. continuously refine the integration discovery process based on real - time insights and developments. review progress and adapt the approach to ensure swift compliance without compromising quality. 7. documentation and knowledge transfer : document every step, change, and implementation made throughout the integration discovery process. make sure your organization and relevant stakeholders understand the insights gained and the solutions implemented. this can be achieved with a solution that visualizes and shows dependencies across your application network. are you tired of hearing about rest, graphql, and asynch apis without really understanding what they mean? well, don\\'t worry ; you\\'re not alone. in the world of application development, there are a lot of different technologies to choose from, and it can be overwhelming to figure out which one is right for your project. so let\\'s break it down. restful apis ; best for simple crud operations first up, we have rest. now, i know what you\\'re thinking, \" representational state transfer, that sounds fancy and confusing! \" but it\\'s really not that complicated. restful apis use http requests to post ( create ), put ( update ), get ( read ), and delete data, and they\\'re based on',\n",
       " 'alone. in the world of application development, there are a lot of different technologies to choose from, and it can be overwhelming to figure out which one is right for your project. so let\\'s break it down. restful apis ; best for simple crud operations first up, we have rest. now, i know what you\\'re thinking, \" representational state transfer, that sounds fancy and confusing! \" but it\\'s really not that complicated. restful apis use http requests to post ( create ), put ( update ), get ( read ), and delete data, and they\\'re based on familiar concepts like urls and http. they are also often used with web services that return data in json or xml format. graphql ; best for complex data requirements next, we have graphql, which is a newer technology developed by facebook. graphql is a query language that allows developers to request exactly the data they need rather than getting a fixed set of data from an endpoint. it\\'s kind of like going to a restaurant and being able to order exactly what you want rather than being forced to order a pre - set meal. this can make it more efficient to work with, especially for more complex',\n",
       " \"on familiar concepts like urls and http. they are also often used with web services that return data in json or xml format. graphql ; best for complex data requirements next, we have graphql, which is a newer technology developed by facebook. graphql is a query language that allows developers to request exactly the data they need rather than getting a fixed set of data from an endpoint. it's kind of like going to a restaurant and being able to order exactly what you want rather than being forced to order a pre - set meal. this can make it more efficient to work with, especially for more complex data requirements. in addition, graphql also provides a type system that allows for more efficient validation and better documentation of the api. asynch apis ; best for real - time applications last but not least, we have asynch apis, which are becoming increasingly popular recently. as the name suggests, these apis work asynchronously, which means that clients can make requests and receive responses as soon as they're available rather than having to wait for a response from the server. this can be really useful for real - time applications like chat or gaming. choosing the api technology that best fits your needs\",\n",
       " \"complex data requirements. in addition, graphql also provides a type system that allows for more efficient validation and better documentation of the api. asynch apis ; best for real - time applications last but not least, we have asynch apis, which are becoming increasingly popular recently. as the name suggests, these apis work asynchronously, which means that clients can make requests and receive responses as soon as they're available rather than having to wait for a response from the server. this can be really useful for real - time applications like chat or gaming. choosing the api technology that best fits your needs so which one is right for your project? well, it depends on your specific needs. if you're working on a simple crud ( create, read, update, delete ) application, restful apis are probably a good choice. if you need more complex data requirements, graphql might be a better fit. and if you're working on a real - time application, asynch apis are definitely worth considering. seek guidance from an integration company if you need help at the end of the day, the most important thing is to evaluate the requirements of your project and choose the technology that best fits your\",\n",
       " \"popular recently. as the name suggests, these apis work asynchronously, which means that clients can make requests and receive responses as soon as they're available rather than having to wait for a response from the server. this can be really useful for real - time applications like chat or gaming. choosing the api technology that best fits your needs so which one is right for your project? well, it depends on your specific needs. if you're working on a simple crud ( create, read, update, delete ) application, restful apis are probably a good choice. if you need more complex data requirements, graphql might be a better fit. and if you're working on a real - time application, asynch apis are definitely worth considering. seek guidance from an integration company if you need help at the end of the day, the most important thing is to evaluate the requirements of your project and choose the technology that best fits your needs. and if you're still feeling overwhelmed, don't hesitate to reach out to an integration company ( like entiros integrations [UNK] ) for guidance. they'll be able to help you navigate the world of apis and choose the right one for your project.\"]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimension of the embeddings\n",
    "dimension = vectors.shape[1]\n",
    "\n",
    "# Create a FAISS index\n",
    "index_DB = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add vectors to the index\n",
    "index_DB.add(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 512 tokens, input is 22 tokens\n"
     ]
    }
   ],
   "source": [
    "# Embed a query text\n",
    "query_text = \"What are the security risks and vulnerabilities associated with point-to-point connectivity in IT infrastructure?\"\n",
    "query_vector, _ = embed_text(query_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 384)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the security risks and vulnerabilities associated with point-to-point connectivity in IT infrastructure?\n",
      "Top 3 most similar texts:\n",
      "1. 5 (Distance: 7.666616916656494) to - point connections'complexity and maintenance overhead. monitoring and troubleshooting each connection individually becomes challenging as the number of connections increases. another limitation is the lack of scalability. point - to - point connections are usually established one - to - one, meaning adding new endpoints requires creating additional connections. this can lead to a tangled web of connections that becomes difficult to manage and scale as the infrastructure grows. furthermore, point - to - point connectivity can result in vendor lock - in. each connection is typically configured with specific protocols and standards, making switching vendors or integrating new technologies challenging. this lack of interoperability can limit the organization's ability to adopt new solutions or take advantage of emerging technologies. lastly, point - to - point connections can also introduce security risks. each connection represents a potential entry point for cyber threats, and securing each connection individually can be a complex task. additionally, point - to - point connections may not provide the security and encryption required to protect sensitive data. security risks and vulnerabilities point - to - point connectivity in it infrastructure can introduce various security risks and vulnerabilities. one of the main concerns is the lack of centralized security controls. with multiple point - to - point connections, enforcing consistent security policies across the entire \n",
      "2. 8 (Distance: 11.29808521270752) regular security audits, and network segmentation to minimize the impact of a potential breach. scalability issues in it infrastructure point - to - point connectivity can pose scalability challenges in it infrastructure. as connections increase, managing and scaling the infrastructure becomes more complex and time - consuming. adding new endpoints requires creating additional connections, resulting in a tangled web of connections that is difficult to manage and maintain. this can lead to increased operational costs and reduced efficiency. moreover, scaling point - to - point connections often requires manual configuration and intervention, which can be time - consuming and error - prone. this manual process can hinder the organization's ability to quickly adapt to changing business requirements and integrate new applications or systems. organizations can leverage self - service integration solutions that automate connectivity to address these scalability issues. organizations can easily add or remove endpoints using self - service integration without manual intervention. this improves scalability, reduces the risk of errors, and accelerates the deployment of new connections. impact on network performance point - to - point connectivity can have a significant impact on network performance. as the number of connections increases, the network can become congested, leading to latency and reduced overall performance. each point - to - point connection consumes network resources, including bandwidth \n",
      "3. 6 (Distance: 11.876526832580566) organization's ability to adopt new solutions or take advantage of emerging technologies. lastly, point - to - point connections can also introduce security risks. each connection represents a potential entry point for cyber threats, and securing each connection individually can be a complex task. additionally, point - to - point connections may not provide the security and encryption required to protect sensitive data. security risks and vulnerabilities point - to - point connectivity in it infrastructure can introduce various security risks and vulnerabilities. one of the main concerns is the lack of centralized security controls. with multiple point - to - point connections, enforcing consistent security policies across the entire network becomes challenging. another risk is the potential exposure of sensitive data during transit. point - to - point connections may not provide adequate encryption or secure transmission protocols, leaving data vulnerable to interception or unauthorized access. furthermore, each point - to - point connection represents a potential entry point for cyber threats. if one connection is compromised, it can expose the entire network to security breaches. this makes it crucial to implement robust security measures at each endpoint and ensure regular monitoring and updates to prevent and detect any potential vulnerabilities. to mitigate these risks, organizations should consider implementing a comprehensive security strategy that includes strong encryption protocols, regular \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform a similarity search\n",
    "k = 3  # Number of nearest neighbors to retrieve\n",
    "distances, indices = index_DB.search(query_vector, k)\n",
    "\n",
    "# Retrieve the most similar texts\n",
    "print(\"Query:\", query_text)\n",
    "print(\"Top 3 most similar texts:\")\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"{i+1}. {idx} (Distance: {distances[0][i]}) {lookup[idx]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 79 tokens\n",
      "\n",
      "User: whome do i speak to?\n",
      "Assistent: You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\n",
      "User: What are the security risks and vulnerabilities associated with point-to-point connectivity in IT infrastructure?\n",
      "Assistent: 1. Point-to-point connectivity is vulnerable to a single point of failure, such as a network outage or hardware failure. This can result in data loss or system downtime. 2. Point-to-point connectivity can also be vulnerable to cyber-attacks, such as malware or hacking. This can result in data theft or unauthorized access to sensitive information. 3. Point-to-point connectivity can also be vulnerable to security breaches, such as leaks of personal or confidential information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = f\"\"\"\n",
    "User: whome do i speak to?\n",
    "Assistent: You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\n",
    "User: {query_text}\n",
    "Assistent: \"\"\"\n",
    "\n",
    "output = generated_text(input_text)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 353 tokens\n",
      "\n",
      "User: whome do i speak to?\n",
      "Assistent: You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\n",
      "User: What do you know about Entiros.\n",
      "Assistent: This is what i found on the wab page of Entiros: to - point connections'complexity and maintenance overhead. monitoring and troubleshooting each connection individually becomes challenging as the number of connections increases. another limitation is the lack of scalability. point - to - point connections are usually established one - to - one, meaning adding new endpoints requires creating additional connections. this can lead to a tangled web of connections that becomes difficult to manage and scale as the infrastructure grows. furthermore, point - to - point connectivity can result in vendor lock - in. each connection is typically configured with specific protocols and standards, making switching vendors or integrating new technologies challenging. this lack of interoperability can limit the organization's ability to adopt new solutions or take advantage of emerging technologies. lastly, point - to - point connections can also introduce security risks. each connection represents a potential entry point for cyber threats, and securing each connection individually can be a complex task. additionally, point - to - point connections may not provide the security and encryption required to protect sensitive data. security risks and vulnerabilities point - to - point connectivity in it infrastructure can introduce various security risks and vulnerabilities. one of the main concerns is the lack of centralized security controls. with multiple point - to - point connections, enforcing consistent security policies across the entire\n",
      "User: What are the security risks and vulnerabilities associated with point-to-point connectivity in IT infrastructure?\n",
      "Assistent: 1. Lack of centralized security controls: With multiple point - to - point connections, enforcing consistent security policies across the entire infrastructure can be challenging. Each connection represents a potential entry point for cyber threats and security vulnerabilities. 2. Inconsistent security policies: With multiple point - to - point connections, enforcing consistent security policies across the entire infrastructure can be difficult. Each connection represents a potential entry point for cyber threats and security vulnerabilities. 3. Lack of centralized security controls: Each point - to\n"
     ]
    }
   ],
   "source": [
    "input_text = f\"\"\"\n",
    "User: whome do i speak to?\n",
    "Assistent: You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\n",
    "User: What do you know about Entiros?\n",
    "Assistent: This is what i found on the wab page of Entiros: {lookup[indices[0][0]]}\n",
    "User: {query_text}\n",
    "Assistent: \"\"\"\n",
    "\n",
    "output = generated_text(input_text)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(230, 384)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruct Models\n",
    "Instruct models are a type of Large Language Model (LLM) specifically trained to follow instructions given in natural language. Unlike general-purpose LLMs, which are trained to predict the next word in a sequence, instruct models are fine-tuned to respond to user prompts by performing specific tasks or answering questions in a way that aligns with the given instructions.\n",
    "\n",
    "* **Instruction-Following:** These models excel at understanding and executing tasks described in plain language prompts.\n",
    "* **Fine-Tuning:** Instruct models are typically fine-tuned on datasets that include pairs of instructions and the desired outputs, making them better at adhering to the user's intent.\n",
    "* **Versatility:** They can handle a wide range of tasks, such as answering questions, generating summaries, providing step-by-step instructions, writing code, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"HuggingFaceTB/SmolLM-360M-Instruct\"\n",
    "\n",
    "inst_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inst_model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "inst_config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 63 tokens\n",
      "<|im_start|>assistent\n",
      "You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you give me a short summary of what Entiros does?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Entiros is a company that specializes in creating and selling digital products, including digital books, e-books, and online courses. They offer a range of content, including fiction, non-fiction, and educational materials, to help people learn and grow.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [{\"role\": \"assistent\", \"content\": \"You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Can you give me a short summary of what Entiros does?\"}]\n",
    "\n",
    "input_text=inst_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "output = generated_text(input_text, tokenizer=inst_tokenizer, model=inst_model, config=inst_config)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 383 tokens\n",
      "<|im_start|>assistent\n",
      "You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.<|im_end|>\n",
      "<|im_start|>user\n",
      "What do you know about Entiros?<|im_end|>\n",
      "<|im_start|>assistent\n",
      "This is what i found on the wab page of Entiros: Better ways to build integrations. Data-informed connectivity. Our approach combines data analysis with experience to make data-informed network decisions, ensuring relevance and effectiveness in every integration.. Data-driven connectivity. Customer case. Find out how adopting technological advancements simplifies processes, boosts expansion, and gears up for upcoming obstacles in the constantly changing automotive sector.. \"Lorem Ipsum text asd asd asd asd asd asd asd asd asd asd asd asd asd asd asd \". Företag ABC. John Johnsson. \"Lorem Ipsum\". Företag ABC. John Johnson. Let's connect. About us. Entiros is a specialized integration company full of integrators, architects, and talented people dedicated to connecting the world.. We offer better ways to build integrations by addressing fundamental industry challenges - both for large companies and people in the industry. We support you through the entire chain, from integration strategy to implementation, management, and operations.. Other services we offer. Starlify. Build better integrations by collaboration and insight. Starlify is a productivity tool for creating smarter connectivity in any IT landscape.. Certified Integrator. Partnerships. Solutions. Event streams\n",
      "Our network. Our network. About us. Integration blog. Connect. An Entiros company. © 2024 Entiros. All rights reserved.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you give me a short summary of what Entiros does?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Entiros is a company that specializes in integrating and connecting various systems, products, and services. They offer a range of solutions to help organizations streamline their operations, improve efficiency, and reduce costs. Here's a brief summary of what Entiros does:\n",
      "\n",
      "**Integration**: Entiros provides integrated solutions that combine multiple systems, products, and services to create a cohesive whole. This can include APIs, microservices, data integration, and more.\n",
      "\n",
      "**\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [{\"role\": \"assistent\", \"content\": \"You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What do you know about Entiros?\"}, \n",
    "            {\"role\": \"assistent\", \"content\": f\"This is what i found on the wab page of Entiros: {web_info}\"},\n",
    "            {\"role\": \"user\", \"content\": \"Can you give me a short summary of what Entiros does?\"}]\n",
    "\n",
    "input_text=inst_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "output = generated_text(input_text, tokenizer=inst_tokenizer, model=inst_model, config=inst_config, max_new_tokens=100)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 68 tokens\n",
      "<|im_start|>assistent\n",
      "You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.<|im_end|>\n",
      "<|im_start|>user\n",
      "What are the security risks and vulnerabilities associated with point-to-point connectivity in IT infrastructure?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Point-to-point connectivity in IT infrastructure can introduce various security risks and vulnerabilities. Here are some of the common ones:\n",
      "\n",
      "1. **Unsecured Data Storage**: Point-to-point connectivity can expose sensitive data to unauthorized access, theft, or loss. This can happen if the data is stored on a shared network or cloud storage service.\n",
      "2. **Insider Threats**: With access to a shared network or cloud storage, an insider with authorized access to the\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [{\"role\": \"assistent\", \"content\": \"You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\"},\n",
    "            {\"role\": \"user\", \"content\": query_text}]\n",
    "input_text=inst_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "output = generated_text(input_text, tokenizer=inst_tokenizer, model=inst_model, config=inst_config, max_new_tokens=100)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 832 tokens\n",
      "<|im_start|>assistent\n",
      "You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.<|im_end|>\n",
      "<|im_start|>user\n",
      "What do you know about Entiros?<|im_end|>\n",
      "<|im_start|>assistent\n",
      "This is what i found on the wab page of Entiros: to - point connections'complexity and maintenance overhead. monitoring and troubleshooting each connection individually becomes challenging as the number of connections increases. another limitation is the lack of scalability. point - to - point connections are usually established one - to - one, meaning adding new endpoints requires creating additional connections. this can lead to a tangled web of connections that becomes difficult to manage and scale as the infrastructure grows. furthermore, point - to - point connectivity can result in vendor lock - in. each connection is typically configured with specific protocols and standards, making switching vendors or integrating new technologies challenging. this lack of interoperability can limit the organization's ability to adopt new solutions or take advantage of emerging technologies. lastly, point - to - point connections can also introduce security risks. each connection represents a potential entry point for cyber threats, and securing each connection individually can be a complex task. additionally, point - to - point connections may not provide the security and encryption required to protect sensitive data. security risks and vulnerabilities point - to - point connectivity in it infrastructure can introduce various security risks and vulnerabilities. one of the main concerns is the lack of centralized security controls. with multiple point - to - point connections, enforcing consistent security policies across the entire\n",
      "regular security audits, and network segmentation to minimize the impact of a potential breach. scalability issues in it infrastructure point - to - point connectivity can pose scalability challenges in it infrastructure. as connections increase, managing and scaling the infrastructure becomes more complex and time - consuming. adding new endpoints requires creating additional connections, resulting in a tangled web of connections that is difficult to manage and maintain. this can lead to increased operational costs and reduced efficiency. moreover, scaling point - to - point connections often requires manual configuration and intervention, which can be time - consuming and error - prone. this manual process can hinder the organization's ability to quickly adapt to changing business requirements and integrate new applications or systems. organizations can leverage self - service integration solutions that automate connectivity to address these scalability issues. organizations can easily add or remove endpoints using self - service integration without manual intervention. this improves scalability, reduces the risk of errors, and accelerates the deployment of new connections. impact on network performance point - to - point connectivity can have a significant impact on network performance. as the number of connections increases, the network can become congested, leading to latency and reduced overall performance. each point - to - point connection consumes network resources, including bandwidth\n",
      "organization's ability to adopt new solutions or take advantage of emerging technologies. lastly, point - to - point connections can also introduce security risks. each connection represents a potential entry point for cyber threats, and securing each connection individually can be a complex task. additionally, point - to - point connections may not provide the security and encryption required to protect sensitive data. security risks and vulnerabilities point - to - point connectivity in it infrastructure can introduce various security risks and vulnerabilities. one of the main concerns is the lack of centralized security controls. with multiple point - to - point connections, enforcing consistent security policies across the entire network becomes challenging. another risk is the potential exposure of sensitive data during transit. point - to - point connections may not provide adequate encryption or secure transmission protocols, leaving data vulnerable to interception or unauthorized access. furthermore, each point - to - point connection represents a potential entry point for cyber threats. if one connection is compromised, it can expose the entire network to security breaches. this makes it crucial to implement robust security measures at each endpoint and ensure regular monitoring and updates to prevent and detect any potential vulnerabilities. to mitigate these risks, organizations should consider implementing a comprehensive security strategy that includes strong encryption protocols, regular<|im_end|>\n",
      "<|im_start|>user\n",
      "What are the security risks and vulnerabilities associated with point-to-point connectivity in IT infrastructure?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Point-to-point connectivity in IT infrastructure can introduce several security risks and vulnerabilities. Here are some of the common ones:\n",
      "\n",
      "1. **Data Exposure**: Point-to-point connections can expose sensitive data, such as passwords, credit card numbers, or personal identifiable information (PII), to unauthorized parties.\n",
      "2. **Unsecured Communication Protocols**: Unencrypted communication protocols, such as SMTP or FTP, can be vulnerable to interception or eavesdropping\n"
     ]
    }
   ],
   "source": [
    "context = \"\\n\".join([ lookup[idx] for idx in indices[0]])\n",
    "messages = [{\"role\": \"assistent\", \"content\": \"You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"What do you know about Entiros?\"},\n",
    "            {\"role\": \"assistent\", \"content\": f\"This is what i found on the wab page of Entiros: {context}\"},\n",
    "            {\"role\": \"user\", \"content\": query_text}]\n",
    "input_text=inst_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "output = generated_text(input_text, tokenizer=inst_tokenizer, model=inst_model, config=inst_config, max_new_tokens=100)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try something completely different\n",
    "\n",
    "We would like to have some structured data because who wants to work with unstructured data can we use LLMs to help us with that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL dummy data\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('data.db')\n",
    "\n",
    "c = conn.cursor()\n",
    "\n",
    "# Create table for animals with columns: id, name, age, species\n",
    "c.execute('''CREATE TABLE animals\n",
    "             (id INTEGER PRIMARY KEY,\n",
    "              name TEXT NOT NULL,\n",
    "              age INTEGER NOT NULL,\n",
    "              species TEXT NOT NULL)''')\n",
    "\n",
    "# Insert data into the table\n",
    "for animal in [(\"Fido\", 4, \"dog\"), (\"Whiskers\", 7, \"cat\"), (\"Fluffy\", 2, \"rabbit\")]:\n",
    "    c.execute(\"INSERT INTO animals (name, age, species) VALUES (?, ?, ?)\", animal)\n",
    "\n",
    "# Save (commit) the changes\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'id', 'INTEGER', 0, None, 1),\n",
       " (1, 'name', 'TEXT', 1, None, 0),\n",
       " (2, 'age', 'INTEGER', 1, None, 0),\n",
       " (3, 'species', 'TEXT', 1, None, 0)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Query meta information about the table\n",
    "\n",
    "c.execute(\"PRAGMA table_info(animals)\")\n",
    "columns = c.fetchall()\n",
    "columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 119 tokens\n",
      "<|im_start|>assistent\n",
      "You are an expert SQL chatbot. Generate ONLY SQL queries what answer user requests. you do not provide any other information.<|im_end|>\n",
      "<|im_start|>user\n",
      " i heve a SQLite database with a table called animals. The table has the following columns: id, name, age, species. Can you give me the SQL query to select the youngest animal?<|im_end|>\n",
      "<|im_start|>assistent\n",
      "SELECT name FROM animals WHERE age = (SELECT MIN(age) FROM animals)<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you give me the SQL query to select the species the oldest animal?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "SELECT species FROM animals WHERE age = (SELECT MAX(age) FROM animals) ORDER BY age ASC<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "message = [{\"role\": \"assistent\", \"content\": \"You are an expert SQL chatbot. Generate ONLY SQL queries what answer user requests. you do not provide any other information.\"},\n",
    "           {\"role\": \"user\", \"content\": f\" i heve a SQLite database with a table called animals. The table has the following columns: {', '.join([c[1] for c in columns])}. Can you give me the SQL query to select the youngest animal?\"}, \n",
    "           {\"role\": \"assistent\", \"content\": \"SELECT name FROM animals WHERE age = (SELECT MIN(age) FROM animals)\"},\n",
    "           {\"role\": \"user\", \"content\": \"Can you give me the SQL query to select the species the oldest animal?\"},\n",
    "           ]\n",
    "input_text=inst_tokenizer.apply_chat_template(message, tokenize=False)\n",
    "\n",
    "output = generated_text(input_text, tokenizer=inst_tokenizer, model=inst_model, config=inst_config, max_new_tokens=100)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('cat',)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Query the database for the youngest animal\n",
    "c.execute(\"SELECT species FROM animals WHERE age = (SELECT MAX(age) FROM animals) ORDER BY age ASC\")\n",
    "youngest_animal = c.fetchone()\n",
    "youngest_animal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to structure some information into a predefined Json structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 370 tokens\n",
      "<|im_start|>assistent\n",
      "You are an expert JSON chatbot. Generate ONLY JSON structure besed on user requests and there story. you do not provide any other information.<|im_end|>\n",
      "<|im_start|>user\n",
      " I have a JSON structure: \n",
      "{\n",
      "  \"name\": string,\n",
      "  \"age\": integer,\n",
      "  \"city\": string,\n",
      "  \"meet\": string,\n",
      "}\n",
      ". During my travels, I had the pleasure of meeting a remarkable person named John. He is a 45-year-old New Yorker with a wealth of stories and experiences that make him truly fascinating. Our paths crossed in a quaint little café tucked away in one of the quieter streets of Manhattan. It was one of those places where the aroma of freshly brewed coffee blends harmoniously with the chatter of locals, creating an ambiance that invites you to sit back and savor the moment. Can you give me the JSON that folows the structure?<|im_end|>\n",
      "<|im_start|>assistent\n",
      "{\"name\": \"John\", \"age\": 45, \"city\": \"New York\", \"meet\": \"café\"}<|im_end|>\n",
      "<|im_start|>user\n",
      " I have a JSON structure: \n",
      "{\n",
      "  \"name\": string,\n",
      "  \"age\": integer,\n",
      "  \"city\": string,\n",
      "  \"meet\": string,\n",
      "}\n",
      ". During the summer, in a small village in the south of France called Saint-Tropez, I met a charming lady named Marie. She is 32 years old and has a passion for painting. Her art is a reflection of her vibrant personality and zest for life. We met at a local art exhibition, where her work was on display. The colors and textures of her paintings captivated me, and I was drawn to the stories they told. Can you give me the JSON that folows the structure?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "{\"name\": \"Marie\", \"age\": 32, \"city\": \"Saint-Tropez\", \"meet\": \"art exhibition\", \"description\": \"colorful paintings\", \"tags\": \"art\", \"tags\": \"vibrant personality\", \"tags\": \"zest for life\", \"tags\": \"vibrant personality\"}<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "json_structure = \"\"\"\n",
    "{\n",
    "  \"name\": string,\n",
    "  \"age\": integer,\n",
    "  \"city\": string,\n",
    "  \"meet\": string,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "message = [{\"role\": \"assistent\", \"content\": \"You are an expert JSON chatbot. Generate ONLY JSON structure besed on user requests and there story. you do not provide any other information.\"},\n",
    "           {\"role\": \"user\", \"content\": f\" I have a JSON structure: {json_structure}. During my travels, I had the pleasure of meeting a remarkable person named John. He is a 45-year-old New Yorker with a wealth of stories and experiences that make him truly fascinating. Our paths crossed in a quaint little café tucked away in one of the quieter streets of Manhattan. It was one of those places where the aroma of freshly brewed coffee blends harmoniously with the chatter of locals, creating an ambiance that invites you to sit back and savor the moment. Can you give me the JSON that folows the structure?\"},\n",
    "           {\"role\": \"assistent\", \"content\": '{\"name\": \"John\", \"age\": 45, \"city\": \"New York\", \"meet\": \"café\"}'},\n",
    "           {\"role\": \"user\", \"content\": f\" I have a JSON structure: {json_structure}. During the summer, in a small village in the south of France called Saint-Tropez, I met a charming lady named Marie. She is 32 years old and has a passion for painting. Her art is a reflection of her vibrant personality and zest for life. We met at a local art exhibition, where her work was on display. The colors and textures of her paintings captivated me, and I was drawn to the stories they told. Can you give me the JSON that folows the structure?\"},\n",
    "           ]\n",
    "\n",
    "input_text=inst_tokenizer.apply_chat_template(message, tokenize=False)\n",
    "\n",
    "output = generated_text(input_text, tokenizer=inst_tokenizer, model=inst_model, config=inst_config, max_new_tokens=100)\n",
    "\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
