{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Large Language Model (LLM): \n",
    "A Large Language Model is an advanced type of artificial intelligence model designed to understand and generate human-like text. \n",
    "These models are trained on vast amounts of text data from diverse sources like books, websites, and articles.\n",
    "\n",
    "* **Context-Based:** LLMs work by analyzing the context in which words appear. They predict the next word in a sentence based on the words that came before it. This ability to understand and generate text based on context makes them incredibly powerful for a wide range of language-related tasks.\n",
    "* **Pre-Trained:** These models are pre-trained on large datasets, meaning they have already learned to recognize patterns, structures, and meanings in text. This pre-training allows them to be used for various tasks with minimal additional training or customization.\n",
    "* **Versatility:** LLMs can perform a wide variety of tasks, such as generating creative writing, answering questions, translating languages, summarizing text, and even understanding the sentiment behind a message.\n",
    "\n",
    "The best way to truly understand LLMs is to experiment with them yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, you'll need to install the required libraries. The Hugging Face transformers library makes it easy for working with LLMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/75/35/07c9879163b603f0e464b0f6e6e628a2340cfc7cdc5ca8e7d52d776710d4/transformers-4.44.2-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torch\n",
      "  Obtaining dependency information for torch from https://files.pythonhosted.org/packages/80/83/9b7681e41e59adb6c2b042f7e8eb716515665a6eed3dda4215c6b3385b90/torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata\n",
      "  Downloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting numpy<2\n",
      "  Obtaining dependency information for numpy<2 from https://files.pythonhosted.org/packages/3a/d0/edc009c27b406c4f9cbc79274d6e46d634d139075492ad055e3d68445925/numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ipywidgets\n",
      "  Obtaining dependency information for ipywidgets from https://files.pythonhosted.org/packages/22/2d/9c0b76f2f9cc0ebede1b9371b6f317243028ed60b90705863d493bae622e/ipywidgets-8.1.5-py3-none-any.whl.metadata\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting requests_html\n",
      "  Obtaining dependency information for requests_html from https://files.pythonhosted.org/packages/24/bc/a4380f09bab3a776182578ce6b2771e57259d0d4dbce178205779abdc347/requests_html-0.10.0-py3-none-any.whl.metadata\n",
      "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting lxml_html_clean\n",
      "  Obtaining dependency information for lxml_html_clean from https://files.pythonhosted.org/packages/5e/09/101504175ffc2c5087c69fac4a3ef82b83609df3c5caa264927a6de176a8/lxml_html_clean-0.2.0-py3-none-any.whl.metadata\n",
      "  Downloading lxml_html_clean-0.2.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting faiss-cpu<1.8\n",
      "  Obtaining dependency information for faiss-cpu<1.8 from https://files.pythonhosted.org/packages/61/e6/3b740163e7fcce1220417ef6c18a7bc00b9d11b64264f3abf483a3771153/faiss_cpu-1.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading faiss_cpu-1.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/ae/f0/48285f0262fe47103a4a45972ed2f9b93e4c80b8fd609fa98da78b2a5706/filelock-3.15.4-py3-none-any.whl.metadata\n",
      "  Downloading filelock-3.15.4-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.23.2 from https://files.pythonhosted.org/packages/b9/8f/d6718641c14d98a5848c6a24d2376028d292074ffade0702940a4b1dde76/huggingface_hub-0.24.6-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Obtaining dependency information for pyyaml>=5.1 from https://files.pythonhosted.org/packages/75/e4/2c27590dfc9992f73aabbeb9241ae20220bd9452df27483b6e56d3975cc5/PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/8b/77/92d4a14530900d46dddc57b728eea65d723cc9fcfd07b96c2c141dabba84/regex-2024.7.24-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2024.7.24-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests (from transformers)\n",
      "  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl.metadata\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.4.1 from https://files.pythonhosted.org/packages/af/b9/c33f69f4dad9c65209efb76c2be6968af5219e31ccfd344a0025d972252f/safetensors-0.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.20,>=0.19 from https://files.pythonhosted.org/packages/a7/03/fb50fc03f86016b227a967c8d474f90230c885c0d18f78acdfda7a96ce56/tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Obtaining dependency information for tqdm>=4.27 from https://files.pythonhosted.org/packages/48/5d/acf5905c36149bbaec41ccf7f2b68814647347b72075ac0b1fe3022fdc73/tqdm-4.66.5-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in ./venv/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Collecting sympy (from torch)\n",
      "  Obtaining dependency information for sympy from https://files.pythonhosted.org/packages/c1/f9/6845bf8fca0eaf847da21c5d5bc6cd92797364662824a11d3f836423a1a5/sympy-1.13.2-py3-none-any.whl.metadata\n",
      "  Downloading sympy-1.13.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Obtaining dependency information for networkx from https://files.pythonhosted.org/packages/38/e9/5f72929373e1a0e8d142a130f3f97e6ff920070f87f91c4e13e40e0fba5a/networkx-3.3-py3-none-any.whl.metadata\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Obtaining dependency information for jinja2 from https://files.pythonhosted.org/packages/31/80/3a54838c3fb461f6fec263ebf3a3a41771bd05190238de3486aae8540c36/jinja2-3.1.4-py3-none-any.whl.metadata\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/5e/44/73bea497ac69bafde2ee4269292fa3b41f1198f4bb7bbaaabde30ad29d4a/fsspec-2024.6.1-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Obtaining dependency information for nvidia-cuda-nvrtc-cu12==12.1.105 from https://files.pythonhosted.org/packages/b6/9f/c64c03f49d6fbc56196664d05dba14e3a561038a81a638eeb47f4d4cfd48/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Obtaining dependency information for nvidia-cuda-runtime-cu12==12.1.105 from https://files.pythonhosted.org/packages/eb/d5/c68b1d2cdfcc59e72e8a5949a37ddb22ae6cade80cd4a57a84d4c8b55472/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Obtaining dependency information for nvidia-cuda-cupti-cu12==12.1.105 from https://files.pythonhosted.org/packages/7e/00/6b218edd739ecfc60524e585ba8e6b00554dd908de2c9c66c1af3e44e18d/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Obtaining dependency information for nvidia-cudnn-cu12==9.1.0.70 from https://files.pythonhosted.org/packages/9f/fd/713452cd72343f682b1c7b9321e23829f00b842ceaedcda96e742ea0b0b3/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Obtaining dependency information for nvidia-cublas-cu12==12.1.3.1 from https://files.pythonhosted.org/packages/37/6d/121efd7382d5b0284239f4ab1fc1590d86d34ed4a4a2fdb13b30ca8e5740/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Obtaining dependency information for nvidia-cufft-cu12==11.0.2.54 from https://files.pythonhosted.org/packages/86/94/eb540db023ce1d162e7bea9f8f5aa781d57c65aed513c33ee9a5123ead4d/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Obtaining dependency information for nvidia-curand-cu12==10.3.2.106 from https://files.pythonhosted.org/packages/44/31/4890b1c9abc496303412947fc7dcea3d14861720642b49e8ceed89636705/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Obtaining dependency information for nvidia-cusolver-cu12==11.4.5.107 from https://files.pythonhosted.org/packages/bc/1d/8de1e5c67099015c834315e333911273a8c6aaba78923dd1d1e25fc5f217/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Obtaining dependency information for nvidia-cusparse-cu12==12.1.0.106 from https://files.pythonhosted.org/packages/65/5b/cfaeebf25cd9fdec14338ccb16f6b2c4c7fa9163aefcf057d86b9cc248bb/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
      "  Obtaining dependency information for nvidia-nccl-cu12==2.20.5 from https://files.pythonhosted.org/packages/4b/2a/0a131f572aa09f741c30ccd45a8e56316e8be8dfc7bc19bf0ab7cfef7b19/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Obtaining dependency information for nvidia-nvtx-cu12==12.1.105 from https://files.pythonhosted.org/packages/da/d3/8057f0587683ed2fcd4dbfbdfdfa807b9160b809976099d36b8f60d08f03/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch)\n",
      "  Obtaining dependency information for triton==3.0.0 from https://files.pythonhosted.org/packages/33/3e/a2f59384587eff6aeb7d37b6780de7fedd2214935e27520430ca9f5b7975/triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata\n",
      "  Downloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Obtaining dependency information for nvidia-nvjitlink-cu12 from https://files.pythonhosted.org/packages/59/65/7ff0569494fbaea45ad2814972cc88da843d53cc96eb8554fcd0908941d9/nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in ./venv/lib/python3.11/site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in ./venv/lib/python3.11/site-packages (from ipywidgets) (8.26.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in ./venv/lib/python3.11/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
      "  Obtaining dependency information for widgetsnbextension~=4.0.12 from https://files.pythonhosted.org/packages/21/02/88b65cc394961a60c43c70517066b6b679738caf78506a5da7b88ffcb643/widgetsnbextension-4.0.13-py3-none-any.whl.metadata\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab-widgets~=3.0.12 (from ipywidgets)\n",
      "  Obtaining dependency information for jupyterlab-widgets~=3.0.12 from https://files.pythonhosted.org/packages/a9/93/858e87edc634d628e5d752ba944c2833133a28fa87bb093e6832ced36a3e/jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata\n",
      "  Downloading jupyterlab_widgets-3.0.13-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting pyquery (from requests_html)\n",
      "  Obtaining dependency information for pyquery from https://files.pythonhosted.org/packages/36/b7/f7ccf9e52e2817e1265d3719c600fa4ef33c07de4d5ef0ced3f43ab1cef2/pyquery-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading pyquery-2.0.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting fake-useragent (from requests_html)\n",
      "  Obtaining dependency information for fake-useragent from https://files.pythonhosted.org/packages/e4/99/60d8cf1b26938c2e0a57e232f7f15641dfcd6f8deda454d73e4145910ff6/fake_useragent-1.5.1-py3-none-any.whl.metadata\n",
      "  Downloading fake_useragent-1.5.1-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting parse (from requests_html)\n",
      "  Obtaining dependency information for parse from https://files.pythonhosted.org/packages/d0/31/ba45bf0b2aa7898d81cbbfac0e88c267befb59ad91a19e36e1bc5578ddb1/parse-1.20.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting bs4 (from requests_html)\n",
      "  Obtaining dependency information for bs4 from https://files.pythonhosted.org/packages/51/bb/bf7aab772a159614954d84aa832c129624ba6c32faa559dfb200a534e50b/bs4-0.0.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Collecting w3lib (from requests_html)\n",
      "  Obtaining dependency information for w3lib from https://files.pythonhosted.org/packages/df/d6/ff9000e85b820ab36c0a93f2c8a4b334a80821b631a56c252aed2d0bd2d3/w3lib-2.2.1-py3-none-any.whl.metadata\n",
      "  Downloading w3lib-2.2.1-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting pyppeteer>=0.0.14 (from requests_html)\n",
      "  Obtaining dependency information for pyppeteer>=0.0.14 from https://files.pythonhosted.org/packages/3d/ee/fb2757a38025421fd3844a0ed0a230b78c9c04a66355024436cf3005a70c/pyppeteer-2.0.0-py3-none-any.whl.metadata\n",
      "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting lxml (from lxml_html_clean)\n",
      "  Obtaining dependency information for lxml from https://files.pythonhosted.org/packages/42/07/b29571a58a3a80681722ea8ed0ba569211d9bb8531ad49b5cacf6d409185/lxml-5.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata\n",
      "  Downloading lxml-5.3.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: decorator in ./venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in ./venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in ./venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in ./venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (2.18.0)\n",
      "Requirement already satisfied: stack-data in ./venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in ./venv/lib/python3.11/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests_html)\n",
      "  Obtaining dependency information for appdirs<2.0.0,>=1.4.3 from https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl.metadata\n",
      "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting certifi>=2023 (from pyppeteer>=0.0.14->requests_html)\n",
      "  Obtaining dependency information for certifi>=2023 from https://files.pythonhosted.org/packages/1c/d5/c84e1a17bf61d4df64ca866a1c9a913874b4e9bdc131ec689a0ad013fb36/certifi-2024.7.4-py3-none-any.whl.metadata\n",
      "  Downloading certifi-2024.7.4-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting importlib-metadata>=1.4 (from pyppeteer>=0.0.14->requests_html)\n",
      "  Obtaining dependency information for importlib-metadata>=1.4 from https://files.pythonhosted.org/packages/c0/14/362d31bf1076b21e1bcdcb0dc61944822ff263937b804a79231df2774d28/importlib_metadata-8.4.0-py3-none-any.whl.metadata\n",
      "  Downloading importlib_metadata-8.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests_html)\n",
      "  Obtaining dependency information for pyee<12.0.0,>=11.0.0 from https://files.pythonhosted.org/packages/16/cc/5cea8a0a0d3deb90b5a0d39ad1a6a1ccaa40a9ea86d793eb8a49d32a6ed0/pyee-11.1.0-py3-none-any.whl.metadata\n",
      "  Downloading pyee-11.1.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests_html)\n",
      "  Obtaining dependency information for urllib3<2.0.0,>=1.25.8 from https://files.pythonhosted.org/packages/ae/6a/99eaaeae8becaa17a29aeb334a18e5d582d873b6f084c11f02581b8d7f7f/urllib3-1.26.19-py2.py3-none-any.whl.metadata\n",
      "  Downloading urllib3-1.26.19-py2.py3-none-any.whl.metadata (49 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests_html)\n",
      "  Obtaining dependency information for websockets<11.0,>=10.0 from https://files.pythonhosted.org/packages/d5/5d/d0b039f0db0bb1fea93437721cf3cd8a244ad02a86960c38a3853d5e1fab/websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting beautifulsoup4 (from bs4->requests_html)\n",
      "  Obtaining dependency information for beautifulsoup4 from https://files.pythonhosted.org/packages/b1/fe/e8c672695b37eecc5cbf43e1d0638d88d66ba3a44c4d321c796f4e59167f/beautifulsoup4-4.12.3-py3-none-any.whl.metadata\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Obtaining dependency information for MarkupSafe>=2.0 from https://files.pythonhosted.org/packages/97/18/c30da5e7a0e7f4603abfc6780574131221d9148f323752c2755d48abad30/MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting cssselect>=1.2.0 (from pyquery->requests_html)\n",
      "  Obtaining dependency information for cssselect>=1.2.0 from https://files.pythonhosted.org/packages/06/a9/2da08717a6862c48f1d61ef957a7bba171e7eefa6c0aa0ceb96a140c2a6b/cssselect-1.2.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/40/26/f35951c45070edc957ba40a5b1db3cf60a9dbb1b350c2d5bef03e01e61de/charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Using cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (33 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Obtaining dependency information for idna<4,>=2.5 from https://files.pythonhosted.org/packages/22/7e/d71db821f177828df9dea8c42ac46473366f191be53080e552e628aad991/idna-3.8-py3-none-any.whl.metadata\n",
      "  Downloading idna-3.8-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Obtaining dependency information for mpmath<1.4,>=1.1.0 from https://files.pythonhosted.org/packages/43/e3/7d92a15f894aa0c9c4b49b8ee9ac9850d6e63b03c9c32c0367a13ae62209/mpmath-1.3.0-py3-none-any.whl.metadata\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting zipp>=0.5 (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests_html)\n",
      "  Obtaining dependency information for zipp>=0.5 from https://files.pythonhosted.org/packages/07/9e/c96f7a4cd0bf5625bb409b7e61e99b1130dc63a98cb8b24aeabae62d43e8/zipp-3.20.1-py3-none-any.whl.metadata\n",
      "  Downloading zipp-3.20.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in ./venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./venv/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->bs4->requests_html)\n",
      "  Obtaining dependency information for soupsieve>1.2 from https://files.pythonhosted.org/packages/d1/c2/fe97d779f3ef3b15f05c94a2f1e3d21732574ed441687474db9d342a7315/soupsieve-2.6-py3-none-any.whl.metadata\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: executing>=1.2.0 in ./venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in ./venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in ./venv/lib/python3.11/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.11/site-packages (from asttokens>=2.1.0->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.0-cp311-cp311-manylinux1_x86_64.whl (797.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.3/797.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.0.0-1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
      "Downloading lxml_html_clean-0.2.0-py3-none-any.whl (13 kB)\n",
      "Downloading faiss_cpu-1.7.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jupyterlab_widgets-3.0.13-py3-none-any.whl (214 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.4/214.4 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.7.24-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (786 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m786.6/786.6 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.4/435.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Downloading fake_useragent-1.5.1-py3-none-any.whl (17 kB)\n",
      "Downloading filelock-3.15.4-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.3/133.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lxml-5.3.0-cp311-cp311-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
      "Downloading pyquery-2.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.13.2-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading w3lib-2.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading certifi-2024.7.4-py3-none-any.whl (162 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.0/163.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached charset_normalizer-3.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (140 kB)\n",
      "Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading idna-3.8-py3-none-any.whl (66 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading importlib_metadata-8.4.0-py3-none-any.whl (26 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading pyee-11.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading urllib3-1.26.19-py2.py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.9/147.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Downloading zipp-3.20.1-py3-none-any.whl (9.0 kB)\n",
      "Installing collected packages: parse, mpmath, fake-useragent, faiss-cpu, appdirs, zipp, widgetsnbextension, websockets, w3lib, urllib3, tqdm, sympy, soupsieve, safetensors, regex, pyyaml, pyee, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, lxml, jupyterlab-widgets, idna, fsspec, filelock, cssselect, charset-normalizer, certifi, triton, requests, pyquery, nvidia-cusparse-cu12, nvidia-cudnn-cu12, lxml_html_clean, jinja2, importlib-metadata, beautifulsoup4, pyppeteer, nvidia-cusolver-cu12, huggingface-hub, bs4, torch, tokenizers, requests_html, ipywidgets, transformers\n",
      "Successfully installed MarkupSafe-2.1.5 appdirs-1.4.4 beautifulsoup4-4.12.3 bs4-0.0.2 certifi-2024.7.4 charset-normalizer-3.3.2 cssselect-1.2.0 faiss-cpu-1.7.4 fake-useragent-1.5.1 filelock-3.15.4 fsspec-2024.6.1 huggingface-hub-0.24.6 idna-3.8 importlib-metadata-8.4.0 ipywidgets-8.1.5 jinja2-3.1.4 jupyterlab-widgets-3.0.13 lxml-5.3.0 lxml_html_clean-0.2.0 mpmath-1.3.0 networkx-3.3 numpy-1.26.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 parse-1.20.2 pyee-11.1.0 pyppeteer-2.0.0 pyquery-2.0.0 pyyaml-6.0.2 regex-2024.7.24 requests-2.32.3 requests_html-0.10.0 safetensors-0.4.4 soupsieve-2.6 sympy-1.13.2 tokenizers-0.19.1 torch-2.4.0 tqdm-4.66.5 transformers-4.44.2 triton-3.0.0 urllib3-1.26.19 w3lib-2.2.1 websockets-10.4 widgetsnbextension-4.0.13 zipp-3.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch \"numpy<2\" ipywidgets requests_html lxml_html_clean \"faiss-cpu<1.8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, AutoModel\n",
    "import torch, faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Generation\n",
    "Let's start with a simple text generation example using the pipeline API from Hugging Face.\n",
    "\n",
    "Note we are using a very small model so that its easier to run on CPU. This will inpact the quality of the generated text. \n",
    "However i prefer to find the lower bound of the model performance. Because if the model can generate good text with a small model, then it will be even better with a larger model. \n",
    "\n",
    "You can find more models at https://huggingface.co/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a79a1ee5194c469f1ceba9bed122fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.69k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61b0163b7e3e498fb5965cde191442c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c92307848645829e2cf05eeaede6e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b89c1f5ec74ed58ab0cca1c04629e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b11d7f31af64bbc9dbe2f2fe1012c69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/831 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a8c7c8cff10495d98b06e066a53b958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/725 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9632f72cc3e4fe5b3427f6925142ef6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.45G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5bc2ff2dc74e099960d15bc5053576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"HuggingFaceTB/SmolLM-360M\"\n",
    "\n",
    "device = \"cpu\" # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "# Display the maximum sequence length (context window size)\n",
    "\n",
    "def generated_text(input_text, tokenizer=tokenizer, model=model, device=device, config=config, max_new_tokens=100):\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    print(f\"Maximum sequence length: {config.max_position_embeddings} tokens, input is {inputs.shape[1]} tokens\")\n",
    "    attention_mask = torch.ones(inputs.shape, device=device)\n",
    "    outputs = model.generate(inputs, max_new_tokens=max_new_tokens, temperature=0.5, do_sample=True, \n",
    "                             pad_token_id=tokenizer.eos_token_id, mask_token_id=tokenizer.mask_token_id, \n",
    "                             eos_token_id=tokenizer.eos_token_id, attention_mask=attention_mask)\n",
    "    output = tokenizer.decode(outputs[0])\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 74 tokens\n",
      "\n",
      "User: whome do i speak to?\n",
      "Assistent: You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\n",
      "User: Can you give me a short summary of what Entiros does?\n",
      "Assistent: 1. Our mission is to provide a comprehensive and accurate resource for individuals seeking information about Entiros. 2. We offer a wide range of articles, reviews, and news articles on the company's products and services. 3. We also provide a platform for users to submit their own articles and reviews.\n",
      "User: How can I get in touch with you?\n",
      "Assistent: 1. Click on the \"Contact Us\" button on our website. 2. Fill out the\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_text = \"\"\"\n",
    "User: whome do i speak to?\n",
    "Assistent: You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\n",
    "User: Can you give me a short summary of what Entiros does?\n",
    "Assistent: \"\"\"\n",
    "output = generated_text(input_text)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context is Everithing\n",
    "\n",
    "* **Context is Crucial:** The more context and details you provide in a prompt, the more accurate and reliable the model's response will be. Always consider what information the model might need to know to generate the best possible answer.\n",
    "\n",
    "* **Prompt Tailoring:** Adjust your prompts according to the specific scenario or task at hand. Whether you’re handling customer service queries, creating content, or seeking coding help, a well-crafted prompt will lead to better outcomes.\n",
    "\n",
    "Let's see if we can improve the answer using a better context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 390 tokens\n",
      "\n",
      "User: whome do i speak to?\n",
      "Assistent: You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\n",
      "User: What do you know about Entiros?\n",
      "Assistent: This is what i found on the wab page of Entiros: Better ways to build integrations. Data-informed connectivity. Our approach combines data analysis with experience to make data-informed network decisions, ensuring relevance and effectiveness in every integration.. Data-driven connectivity. Customer case. Find out how adopting technological advancements simplifies processes, boosts expansion, and gears up for upcoming obstacles in the constantly changing automotive sector.. \"Lorem Ipsum text asd asd asd asd asd asd asd asd asd asd asd asd asd asd asd \". Företag ABC. John Johnsson. \"Lorem Ipsum\". Företag ABC. John Johnson. Let's connect. About us. Entiros is a specialized integration company full of integrators, architects, and talented people dedicated to connecting the world.. We offer better ways to build integrations by addressing fundamental industry challenges - both for large companies and people in the industry. We support you through the entire chain, from integration strategy to implementation, management, and operations.. Other services we offer. Starlify. Build better integrations by collaboration and insight. Starlify is a productivity tool for creating smarter connectivity in any IT landscape.. Certified Integrator. Partnerships. Solutions. Event streams\n",
      "Our network. Our network. About us. Integration blog. Connect. An Entiros company. © 2024 Entiros. All rights reserved.\n",
      "User: Can you give me a short summary of what Entiros does?\n",
      "Assistent: 1) We understand the challenges of integrating new technologies and data into existing systems. 2) We provide a comprehensive solution that combines data analytics, automation, and automation, ensuring a seamless integration process. 3) We offer a wide range of services, including integration, automation, and automation, to help businesses connect and collaborate effectively. 4) We leverage our expertise in data analysis, automation, and automation to provide a reliable and efficient integration solution.. 5) We are committed to staying updated\n"
     ]
    }
   ],
   "source": [
    "import requests_html\n",
    "\n",
    "session = requests_html.HTMLSession()\n",
    "url = \"https://www.entiros.se/\"\n",
    "response = session.get(url)\n",
    "# get all the text from the p abd h tags\n",
    "web_info = response.html.xpath('//p | //h1 | //h2 | //h3 | //h4 | //h5 | //h6 | //li')\n",
    "web_info = [t.text for t in web_info]\n",
    "web_info = \". \".join(web_info)\n",
    "\n",
    "input_text = f\"\"\"\n",
    "User: whome do i speak to?\n",
    "Assistent: You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\n",
    "User: What do you know about Entiros?\n",
    "Assistent: This is what i found on the wab page of Entiros: {web_info}\n",
    "User: Can you give me a short summary of what Entiros does?\n",
    "Assistent: \"\"\"\n",
    "\n",
    "output = generated_text(input_text)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Are Tokens?\n",
    "In the context of Large Language Models (LLMs), tokens are the individual pieces of text that the model processes. A token can be as small as a single character, like a letter or punctuation mark, or as large as a full word or subword, depending on the specific tokenization method used by the model.\n",
    "\n",
    "* **Tokenization:** Before an LLM can process text, it needs to break the text down into tokens. For example, the sentence \"Hello, world!\" might be tokenized into [\"Hello\", \",\", \"world\", \"!\"]. The way text is split into tokens depends on the model and the tokenizer associated with it.\n",
    "Processing Text: The model then processes these tokens sequentially, using them to predict the next token or generate a response based on the sequence of tokens it has seen so far.\n",
    "What Is the Context Window?\n",
    "The context window refers to the maximum number of tokens that an LLM can process at one time. This is also known as the model's \"maximum sequence length.\" For instance, if an LLM has a context window of 512 tokens, it can only consider the most recent 512 tokens when generating its next output.\n",
    "\n",
    "* **Context Window Size:** The size of the context window is crucial because it determines how much information the model can keep track of at once. A larger context window allows the model to consider more context when generating responses, which can lead to more coherent and contextually relevant outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19556, 28, 10369, 89, 4066, 17]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a text\n",
    "ids = tokenizer.encode(\"Hello, Entiros!\")\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello -> 19556\n",
      ", -> 28\n",
      " Ent -> 10369\n",
      "i -> 89\n",
      "ros -> 4066\n",
      "! -> 17\n"
     ]
    }
   ],
   "source": [
    "# ids to text representation\n",
    "for id in ids:\n",
    "    print(f\"{tokenizer.decode(id)} -> {id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://blog.entiros.se/en/blog-media/from-data-lakes-to-dynamic-data-ecosystems\n",
      "https://blog.entiros.se/en/blog-media/the-challenges-of-point-to-point-connectivity-in-it-infrastructure\n",
      "https://blog.entiros.se/en/blog-media/the-importance-of-data-for-business-intelligence\n",
      "https://blog.entiros.se/en/blog-media/entiros-launches-starlify-in-swedish-cloud-service-approved-by-skr\n",
      "https://blog.entiros.se/en/blog-media/7-ways-integration-discovery-can-enhance-your-psd2-compliance\n",
      "https://blog.entiros.se/en/blog-media/choosing-the-right-api-technology\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The role of data lakes is undergoing a significant transformation. Traditionally viewed as mere repositories for storing vast amounts of data, the modern approach to utilizing data lakes is much more dynamic and strategic, particularly when enhancing Business Intelligence (BI) operations.\\nThe Traditional Data Lake: A Repository, Not a Source\\nHistorically, data lakes have been treated as data sinks — places where data is accumulated from various sources but seldom retrieved for real-time analysis. The common perception was that once data was stored in a data lake, it was not to be disturbed except for predefined reporting and BI tasks. This originated from concerns that the data, often batch-processed from dozens if not hundreds of applications, was too stale for real-time decision-making.\\nHowever, a paradigm shift is now reshaping how organizations approach their integration networks. Companies are moving towards real-time data flow systems rather than relying on traditional Extract, Transform, Load (ETL) methods, which often result in delays and outdated information. This shift means that data now travels from creation to data lake in seconds, significantly refreshing the quality and timeliness of the information stored.\\nUtilizing Data Lakes as Active Databases\\nWith this new approach, data lakes are transitioning from static data repositories to active, operational databases. This shift allows for implementing RESTful operations directly over the data lake, transforming it into a highly responsive and scalable data management system. Such capabilities enable organizations to store data and interact with it dynamically to drive real-time business intelligence and analytics.\\nImplementing Advanced Data Strategies\\nOrganizations can gather and compile data more effectively by accessing multiple data sources, including real-time streams from a data lake. The ability to tap into a centrally managed yet highly accessible data lake allows for more robust and scalable integration of data sources, facilitating more comprehensive and agile BI solutions.\\nThe Impact on Business Intelligence\\nFor BI teams, the implications of these advancements are profound. Instead of pulling outdated data for analysis, BI professionals can now access the most current data streams, enabling more accurate and timely insights. This capability enhances the quality of business decisions and supports more sophisticated analytics, such as predictive modeling and real-time analytics.\\nMoreover, integrating real-time data flows helps bridge the gap between different data domains within an organization, fostering better collaboration and data sharing among departments. As data flows become more integrated and responsive, the entire organization benefits from a more unified and agile approach to data management.\\nLooking Ahead\\nAs we look to the future, the role of data lakes\\xa0is set\\xa0to become even more central in enterprises’ data ecosystems. With advancements in technology and integration strategies, data lakes are poised to become not just storage facilities but vital components of an enterprise’s data architecture - powering innovation and operational efficiency.\\n\\n Explore the common challenges faced in IT infrastructure when using point-to-point connectivity and learn how to overcome them. How can Self-Service Integration solve many of the challenges?\\nPoint-to-Point Connectivity Limitations\\nWhile point-to-point connectivity offers direct communication between two endpoints, it has certain limitations that can impact IT infrastructure. One major limitation is managing multiple point-to-point connections\\' complexity and maintenance overhead. Monitoring and troubleshooting each connection individually becomes challenging as the number of connections increases.\\nAnother limitation is the lack of scalability. Point-to-point connections are usually established one-to-one, meaning adding new endpoints requires creating additional connections. This can lead to a tangled web of connections that becomes difficult to manage and scale as the infrastructure grows.\\nFurthermore, point-to-point connectivity can result in vendor lock-in. Each connection is typically configured with specific protocols and standards, making switching vendors or integrating new technologies challenging. This lack of interoperability can limit the organization\\'s ability to adopt new solutions or take advantage of emerging technologies.\\nLastly, point-to-point connections can also introduce security risks. Each connection represents a potential entry point for cyber threats, and securing each connection individually can be a complex task. Additionally, point-to-point connections may not provide the security and encryption required to protect sensitive data.\\nSecurity Risks and Vulnerabilities\\nPoint-to-point connectivity in IT infrastructure can introduce various security risks and vulnerabilities. One of the main concerns is the lack of centralized security controls. With multiple point-to-point connections, enforcing consistent security policies across the entire network becomes challenging.\\nAnother risk is the potential exposure of sensitive data during transit. Point-to-point connections may not provide adequate encryption or secure transmission protocols, leaving data vulnerable to interception or unauthorized access.\\nFurthermore, each point-to-point connection represents a potential entry point for cyber threats. If one connection is compromised, it can expose the entire network to security breaches. This makes it crucial to implement robust security measures at each endpoint and ensure regular monitoring and updates to prevent and detect any potential vulnerabilities.\\nTo mitigate these risks, organizations should consider implementing a comprehensive security strategy that includes strong encryption protocols, regular security audits, and network segmentation to minimize the impact of a potential breach.\\nScalability Issues in IT Infrastructure\\nPoint-to-point connectivity can pose scalability challenges in IT infrastructure. As connections increase, managing and scaling the infrastructure becomes more complex and time-consuming.\\nAdding new endpoints requires creating additional connections, resulting in a tangled web of connections that is difficult to manage and maintain. This can lead to increased operational costs and reduced efficiency.\\nMoreover, scaling point-to-point connections often requires manual configuration and intervention, which can be time-consuming and error-prone. This manual process can hinder the organization\\'s ability to quickly adapt to changing business requirements and integrate new applications or systems.\\nOrganizations can leverage self-service integration solutions that automate connectivity to address these scalability issues. Organizations can easily add or remove endpoints using self-service integration without manual intervention. This improves scalability, reduces the risk of errors, and accelerates the deployment of new connections.\\nImpact on Network Performance\\nPoint-to-point connectivity can have a significant impact on network performance. As the number of connections increases, the network can become congested, leading to latency and reduced overall performance.\\nEach point-to-point connection consumes network resources, including bandwidth and processing power. This can result in network bottlenecks and decreased performance for other applications and users.\\nFurthermore, managing and monitoring multiple point-to-point connections can be resource-intensive and require dedicated network administrators. This can divert resources from other critical tasks and impact overall network efficiency.\\nOrganizations should consider implementing alternative connectivity approaches, such as using a hub-and-spoke or mesh network topology to mitigate the impact on network performance. These approaches provide centralized management and better utilization of network resources, resulting in improved performance and scalability.\\nBenefits of Leveraging Self-Service Integration to Overcome P2P Obstacles\\nPoint-to-point connectivity in IT infrastructure can present several challenges, but leveraging self-service integration can help overcome these obstacles. By using self-service integration, organizations can streamline their connectivity processes and reduce the need for manual intervention. This saves time and resources and minimizes the risk of errors that can occur during manual configuration.\\nSelf-service integration also enables better collaboration between different systems and applications. It allows seamless data exchange and synchronization, facilitating efficient communication between various IT infrastructure components. This can improve overall productivity and enhance the user experience.\\nAdditionally, self-service integration provides greater flexibility and agility in adapting to changing business requirements. It allows organizations to quickly integrate new applications or systems into their infrastructure without disrupting the entire network. This scalability and adaptability can greatly benefit organizations in today\\'s fast-paced and dynamic business environment.\\n\\n Today, the Integration Center is working very well in describing its application networks and building integrations, and it has many different parties that need data. However, one department in the company needs data more than anyone else - the BI department.\\nThe Business Intelligence department might sit with a data lake and a BI function. Many are still working with analytical BI, where one might look at the last week\\'s data, the last month\\'s data, and, in the best case, the last day\\'s data. Very few are working with operational BI yet, meaning they don\\'t understand what is happening in the reports. How do we act in our business right now to meet certain needs?\\nThe integration centers play an extremely important role here. The BI team needs more data than any other team, and they need current data. Therefore, it is necessary to supplement batch data loading with streaming data. New solutions are needed, for example, based on Kafka instead of using ETL tools.\\nWhat is particularly interesting now is that the BI team is not only working with business intelligence; they have started working with artificial intelligence, or AI. An extremely important aspect of this is the availability of data. You can only train a good model if you have data to train on. The companies that have not yet started collecting data in their data lakes will not have any data to train on. If we see the BI development continue as quickly as it is now, including the specific area of AI, there will be a watershed within maybe a year. The companies that have data to apply their AI on and those that do NOT have data to apply their AI on. And this will then be decided by not how good a BI product you have but how good an integration product you have. How much data have you transported from your application network to your BI team?\\nWhat is required of the BI team for this to happen?\\nThey need to understand how the company\\'s application network looks.\\nWhere is data located?\\nWhat data are we transporting today?\\nWhat data SHOULD we be transporting today to be able to do better BI and even AI tomorrow?\\nBI and integration managers need to talk more to each other. This is about today\\'s challenge of achieving operational BI and tomorrow\\'s challenge of getting data to train on for the coming AI function.\\nOtherwise, there will be no cognitive systems in the company.\\nNo data - no cognition.\\n\\n In a groundbreaking move, Entiros unveiled Starlify, a cutting-edge solution designed to address public sector organizations\\' pervasive challenges in managing their system landscapes. The lack of clarity and understanding of the current state often hinders effective decision-making, leading to increased costs and inefficiencies in IT projects.\\nStarlify Launches in Swedish Cloud Service Endorsed by SKR\\nEntiros proudly announces the official launch of Starlify in a Swedish cloud service approved by SKR (Swedish Association of Local Authorities and Regions). This strategic move positions Starlify as a secure and reliable solution for public sector entities in Sweden.\\nRevolutionizing System Overview and Efficiency\\nPublic sector organizations have started using the tool to find dependencies and improve efficiency. From before, among others, Volvo Cars, with one of Sweden\\'s largest application networks, uses Starlify. Entiros is now looking forward to further collaborations to support Sweden\\'s municipalities and regions in their challenges.\\nAddressing Critical Issues in Public Sector IT\\nThe struggle to understand the system\\'s current state in the public sector often results in ineffective strategies, resource wastage, and budget breaches. In addition, it becomes difficult to see and follow the benefits and effects of the projects. As an example, over SEK 3 billion (2018), out of a total of SEK 9.4 billion, went to projects where the benefit or effect could not be followed after the end of the project. This ultimately impacts the ability to deliver high-quality services to the community. Starlify aims to rectify this by providing a user-friendly tool to collect and create comprehensive overviews of systems and applications.\\nKey Features and User Benefits\\nStarlify facilitates quick mapping and visualization, enabling efficient planning before implementing changes. This occurs within a secure environment that fosters collaboration between internal and external suppliers. Crucially, the tool supports the creation of efficient management processes for sensitive information, ensuring compliance with regulations.\\nAnders Holmstrand, Product Manager at Starlify, Comments:\\n\"Starlify empowers public sector organizations to take control of their system landscape and effectively manage changes with insight and security. Our presence at the Solutions for Public Sector trade show in January offers an excellent opportunity for visitors to learn more about our tool and discuss their challenges in IT projects and change work.\"\\n\\n What is PSD2?\\nPSD2, short for the Second Payment Services Directive, aims to revolutionize payment services. This regulation opens consumer bank accounts to third-party providers (TPPs). Unlocking vast amounts of data banks hold. It levels the playing field and bridges the gap between traditional banks, fintech, and big techs. It represents a shift in the European banking sector, moving it toward the era of Open Banking.\\nHow it Affects Banks\\nThe impact of PSD2 on banks is monumental. This regulation grants third-party providers access to bank data. However, its implementation faced hurdles, experiencing delays and partial enforcement in different countries. Two of the core aspects of PSD2 faced challenges in full deployment. These were strong customer authentication (SCA) and standardized APIs for account interfaces. The banks navigated through these complexities while trying to adapt to the regulatory landscape.\\nWhy Must Banks Adapt Integration Landscapes for PSD2 Compliance?\\nThe challenge for banks is twofold: achieving compliance while maintaining technological readiness. This requires meeting defined standards and preparing for potential revisions and expansions. Banks must balance regulatory compliance demands with the agility needed to adapt. The adaptation process is a continuous journey that\\'s necessary to secure compliance. This ongoing adjustment ensures that banks are ready for upcoming regulatory changes.\\nIntegration Discovery for PSD2 Compliance\\nImplementing an integration solution for PSD2 compliance requires a strategic approach. An approach that guarantees efficiency and rapid adaptability. Banks and other financial institutions must chart a straightforward roadmap to navigate both swiftly and effectively. Otherwise, it\\'s unlikely that the PSD2 compliance will succeed. How do they do it? Integration discovery.\\n1. Strategic Alignment and Objectives Setting:\\nPrioritize aligning your integration strategy with the overarching goals of PSD2 compliance. Set clear objectives and outline the wanted outcomes from the integration process. Establish a dedicated team with IT, compliance, security, and business strategy experts to work with the initiative.\\n2. Comprehensive Landscape Assessment:\\nInitiate with a complete assessment of your current integration landscape. Engage stakeholders, IT teams, and relevant departments to understand your current state. That involves existing systems, APIs, data architecture, security protocols, and integrations. Document and analyze the current state and potential areas for improvement. HÄR KAN STARLIFY HJÄLPA TILL.\\n3. Technology Stack Evaluation:\\nEvaluate your technology stack against PSD2 compliance requirements. Identify any gaps between the existing technology stack and the compliance prerequisites.\\n4. Collaboration and Partnership Exploration:\\nFoster collaborations and partnerships within the industry and engage with both third-party providers and internal experts to leverage their expertise and experiences. Explore collaborations that can expedite the implementation of necessary changes and adaptations to meet compliance requirements. And, of course, collaborations that can help you get an overview of your current state.\\n5. Creation of a Rapid Deployment Plan:\\nUtilize the insights from the assessment and analysis phases to devise a rapid deployment plan. Prioritize actions that address critical gaps and compliance necessities. Craft a phased approach, allowing incremental improvements while aligning with regulatory timelines.\\n6. Implementation and Continuous Iteration:\\nImplement the planned changes and adaptations in a phased manner. Continuously refine the integration discovery process based on real-time insights and developments. Review progress and adapt the approach to ensure swift compliance without compromising quality.\\n7. Documentation and Knowledge Transfer:\\nDocument every step, change, and implementation made throughout the integration discovery process. Make sure your organization and relevant stakeholders understand the insights gained and the solutions implemented. This can be achieved with a solution that visualizes and shows dependencies across your application network.\\n\\n Are you tired of hearing about REST, GraphQL, and Asynch APIs without really understanding what they mean? Well, don\\'t worry; you\\'re not alone. In the world of application development, there are a lot of different technologies to choose from, and it can be overwhelming to figure out which one is right for your project. So let\\'s break it down.\\nRESTful APIs; Best for simple CRUD operations\\nFirst up, we have REST. Now, I know what you\\'re thinking, \"Representational State Transfer, that sounds fancy and confusing!\" But it\\'s really not that complicated. RESTful APIs use HTTP requests to POST (create), PUT (update), GET (read), and DELETE data, and they\\'re based on familiar concepts like URLs and HTTP. They are also often used with web services that return data in JSON or XML format.\\nGraphQL; Best for complex data requirements\\nNext, we have GraphQL, which is a newer technology developed by Facebook. GraphQL is a query language that allows developers to request exactly the data they need rather than getting a fixed set of data from an endpoint. It\\'s kind of like going to a restaurant and being able to order exactly what you want rather than being forced to order a pre-set meal. This can make it more efficient to work with, especially for more complex data requirements. In addition, GraphQL also provides a type system that allows for more efficient validation and better documentation of the API.\\nAsynch APIs; Best for real-time applications\\nLast but not least, we have Asynch APIs, which are becoming increasingly popular recently. As the name suggests, these APIs work asynchronously, which means that clients can make requests and receive responses as soon as they\\'re available rather than having to wait for a response from the server. This can be really useful for real-time applications like chat or gaming.\\nChoosing the API technology that best fits your needs\\nSo which one is right for your project? Well, it depends on your specific needs. If you\\'re working on a simple CRUD (create, read, update, delete) application, RESTful APIs are probably a good choice. If you need more complex data requirements, GraphQL might be a better fit. And if you\\'re working on a real-time application, Asynch APIs are definitely worth considering.\\nSeek guidance from an integration company if you need help\\nAt the end of the day, the most important thing is to evaluate the requirements of your project and choose the technology that best fits your needs. And if you\\'re still feeling overwhelmed, don\\'t hesitate to reach out to an integration company (like Entiros Integrations 😎) for guidance. They\\'ll be able to help you navigate the world of APIs and choose the right one for your project.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://blog.entiros.se/en/blog-media\"\n",
    "response = session.get(url)\n",
    "blogs = response.html.xpath(r\"//a[contains(@class, 'blog-post__post')]\")\n",
    "texts = []\n",
    "for item in blogs:\n",
    "    print(item.attrs['href'])\n",
    "    response = session.get(item.attrs['href'])\n",
    "    blog_text = response.html.xpath(r\"//article\")\n",
    "    texts.append(blog_text[0].text)\n",
    "\n",
    "policy_info = \"\\n\\n \".join(texts)\n",
    "policy_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 3844 tokens\n"
     ]
    }
   ],
   "source": [
    "encoded_wiki_info = tokenizer.encode(policy_info, return_tensors=\"pt\").to(device)\n",
    "print(f\"Maximum sequence length: {config.max_position_embeddings} tokens, input is {encoded_wiki_info.shape[1]} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What can we do when the input is too long?\n",
    "When the input text is too long, it can exceed the model's context window size, leading to incomplete or inaccurate responses. In such cases, you can try the following strategies to handle long inputs:\n",
    "\n",
    "* **Summarization:** Use a text summarization model to condense the input text into a shorter summary that captures the key points. Feed the summarized text into the LLM for more concise and focused responses.\n",
    "* **Rephrasing:** Try rephrasing the input text to convey the same information more concisely. This can help reduce the length of the input while retaining its essential meaning.\n",
    "* **Context Management:** Focus on providing the most relevant and critical information in the input prompt. Remove unnecessary details or background information to streamline the input and improve the model's response quality.\n",
    "\n",
    "Today we will focus on the information retrieval that is the most important part of the context management. \n",
    "Information retrieval is all about clustering the information based on the context. When we have the clusters we can retrieve the information that is most similar to the context.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1235e94d8a2043cf88a63ac5cee6fed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9173dc3704714d5dab49d434376c7515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e9f5a8148747d5ac8461fbd24d2288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc8586db4fe648eeaa0c45ac7e6492ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arty/workshop_llms/venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb4ec359fc44c9d8dd913c21f4ad174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cabd99a97e434304acef326b874aa89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "embed_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embed_tokenizer = AutoTokenizer.from_pretrained(embed_model_name)\n",
    "embed_model = AutoModel.from_pretrained(embed_model_name)\n",
    "embed_config = AutoConfig.from_pretrained(embed_model_name)\n",
    "max_position_embeddings = embed_config.max_position_embeddings\n",
    "\n",
    "def embed_text(text, token_length=max_position_embeddings, overlap=max_position_embeddings//2):\n",
    "    # Tokenize the input text\n",
    "    inputs = embed_tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    print(f\"Maximum sequence length: {max_position_embeddings} tokens, input is {inputs['input_ids'].shape[1]} tokens\")\n",
    "\n",
    "    # Split the input text into smaller parts\n",
    "    input_list = inputs[\"input_ids\"].reshape(-1).tolist()\n",
    "    cls = embed_tokenizer.cls_token_id\n",
    "    n = len(input_list)\n",
    "\n",
    "    input_ids = [ [cls] + input_list[i:i+token_length-1] for i in range(0, n-token_length-1, token_length-overlap-1)] # this will drop the last part \n",
    "    input_ids.append([cls] + input_list[-token_length+1:]) # add the last part\n",
    "    lookup = [embed_tokenizer.decode(ids[1:]) for ids in input_ids]\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "\n",
    "    # Get the embeddings\n",
    "    with torch.no_grad():\n",
    "        embeddings = embed_model(input_ids).last_hidden_state[:, 0, :]\n",
    "\n",
    "    # Convert embeddings to numpy arrays\n",
    "    return embeddings.numpy(), lookup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3868 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 512 tokens, input is 3868 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30, 384)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "vectors, lookup = embed_text(policy_info, token_length=256, overlap=128)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the role of data lakes is undergoing a significant transformation. traditionally viewed as mere repositories for storing vast amounts of data, the modern approach to utilizing data lakes is much more dynamic and strategic, particularly when enhancing business intelligence ( bi ) operations. the traditional data lake : a repository, not a source historically, data lakes have been treated as data sinks — places where data is accumulated from various sources but seldom retrieved for real - time analysis. the common perception was that once data was stored in a data lake, it was not to be disturbed except for predefined reporting and bi tasks. this originated from concerns that the data, often batch - processed from dozens if not hundreds of applications, was too stale for real - time decision - making. however, a paradigm shift is now reshaping how organizations approach their integration networks. companies are moving towards real - time data flow systems rather than relying on traditional extract, transform, load ( etl ) methods, which often result in delays and outdated information. this shift means that data now travels from creation to data lake in seconds, significantly refreshing the quality and timeliness of the information stored. utilizing data lakes as active databases with this new approach, data lakes are transitioning from static data repositories to active',\n",
       " ', often batch - processed from dozens if not hundreds of applications, was too stale for real - time decision - making. however, a paradigm shift is now reshaping how organizations approach their integration networks. companies are moving towards real - time data flow systems rather than relying on traditional extract, transform, load ( etl ) methods, which often result in delays and outdated information. this shift means that data now travels from creation to data lake in seconds, significantly refreshing the quality and timeliness of the information stored. utilizing data lakes as active databases with this new approach, data lakes are transitioning from static data repositories to active, operational databases. this shift allows for implementing restful operations directly over the data lake, transforming it into a highly responsive and scalable data management system. such capabilities enable organizations to store data and interact with it dynamically to drive real - time business intelligence and analytics. implementing advanced data strategies organizations can gather and compile data more effectively by accessing multiple data sources, including real - time streams from a data lake. the ability to tap into a centrally managed yet highly accessible data lake allows for more robust and scalable integration of data sources, facilitating more comprehensive and agile bi solutions. the impact on business intelligence for bi teams, the',\n",
       " 'active, operational databases. this shift allows for implementing restful operations directly over the data lake, transforming it into a highly responsive and scalable data management system. such capabilities enable organizations to store data and interact with it dynamically to drive real - time business intelligence and analytics. implementing advanced data strategies organizations can gather and compile data more effectively by accessing multiple data sources, including real - time streams from a data lake. the ability to tap into a centrally managed yet highly accessible data lake allows for more robust and scalable integration of data sources, facilitating more comprehensive and agile bi solutions. the impact on business intelligence for bi teams, the implications of these advancements are profound. instead of pulling outdated data for analysis, bi professionals can now access the most current data streams, enabling more accurate and timely insights. this capability enhances the quality of business decisions and supports more sophisticated analytics, such as predictive modeling and real - time analytics. moreover, integrating real - time data flows helps bridge the gap between different data domains within an organization, fostering better collaboration and data sharing among departments. as data flows become more integrated and responsive, the entire organization benefits from a more unified and agile approach to data management. looking ahead as we look to the future, the role of data',\n",
       " 'the implications of these advancements are profound. instead of pulling outdated data for analysis, bi professionals can now access the most current data streams, enabling more accurate and timely insights. this capability enhances the quality of business decisions and supports more sophisticated analytics, such as predictive modeling and real - time analytics. moreover, integrating real - time data flows helps bridge the gap between different data domains within an organization, fostering better collaboration and data sharing among departments. as data flows become more integrated and responsive, the entire organization benefits from a more unified and agile approach to data management. looking ahead as we look to the future, the role of data lakes is set to become even more central in enterprises ’ data ecosystems. with advancements in technology and integration strategies, data lakes are poised to become not just storage facilities but vital components of an enterprise ’ s data architecture - powering innovation and operational efficiency. explore the common challenges faced in it infrastructure when using point - to - point connectivity and learn how to overcome them. how can self - service integration solve many of the challenges? point - to - point connectivity limitations while point - to - point connectivity offers direct communication between two endpoints, it has certain limitations that can impact it infrastructure. one major limitation is managing multiple point - to',\n",
       " \"data lakes is set to become even more central in enterprises ’ data ecosystems. with advancements in technology and integration strategies, data lakes are poised to become not just storage facilities but vital components of an enterprise ’ s data architecture - powering innovation and operational efficiency. explore the common challenges faced in it infrastructure when using point - to - point connectivity and learn how to overcome them. how can self - service integration solve many of the challenges? point - to - point connectivity limitations while point - to - point connectivity offers direct communication between two endpoints, it has certain limitations that can impact it infrastructure. one major limitation is managing multiple point - to - point connections'complexity and maintenance overhead. monitoring and troubleshooting each connection individually becomes challenging as the number of connections increases. another limitation is the lack of scalability. point - to - point connections are usually established one - to - one, meaning adding new endpoints requires creating additional connections. this can lead to a tangled web of connections that becomes difficult to manage and scale as the infrastructure grows. furthermore, point - to - point connectivity can result in vendor lock - in. each connection is typically configured with specific protocols and standards, making switching vendors or integrating new technologies challenging. this lack of interoperability can limit the organization\",\n",
       " \"to - point connections'complexity and maintenance overhead. monitoring and troubleshooting each connection individually becomes challenging as the number of connections increases. another limitation is the lack of scalability. point - to - point connections are usually established one - to - one, meaning adding new endpoints requires creating additional connections. this can lead to a tangled web of connections that becomes difficult to manage and scale as the infrastructure grows. furthermore, point - to - point connectivity can result in vendor lock - in. each connection is typically configured with specific protocols and standards, making switching vendors or integrating new technologies challenging. this lack of interoperability can limit the organization's ability to adopt new solutions or take advantage of emerging technologies. lastly, point - to - point connections can also introduce security risks. each connection represents a potential entry point for cyber threats, and securing each connection individually can be a complex task. additionally, point - to - point connections may not provide the security and encryption required to protect sensitive data. security risks and vulnerabilities point - to - point connectivity in it infrastructure can introduce various security risks and vulnerabilities. one of the main concerns is the lack of centralized security controls. with multiple point - to - point connections, enforcing consistent security policies across the entire\",\n",
       " \"organization's ability to adopt new solutions or take advantage of emerging technologies. lastly, point - to - point connections can also introduce security risks. each connection represents a potential entry point for cyber threats, and securing each connection individually can be a complex task. additionally, point - to - point connections may not provide the security and encryption required to protect sensitive data. security risks and vulnerabilities point - to - point connectivity in it infrastructure can introduce various security risks and vulnerabilities. one of the main concerns is the lack of centralized security controls. with multiple point - to - point connections, enforcing consistent security policies across the entire network becomes challenging. another risk is the potential exposure of sensitive data during transit. point - to - point connections may not provide adequate encryption or secure transmission protocols, leaving data vulnerable to interception or unauthorized access. furthermore, each point - to - point connection represents a potential entry point for cyber threats. if one connection is compromised, it can expose the entire network to security breaches. this makes it crucial to implement robust security measures at each endpoint and ensure regular monitoring and updates to prevent and detect any potential vulnerabilities. to mitigate these risks, organizations should consider implementing a comprehensive security strategy that includes strong encryption protocols, regular\",\n",
       " \"entire network becomes challenging. another risk is the potential exposure of sensitive data during transit. point - to - point connections may not provide adequate encryption or secure transmission protocols, leaving data vulnerable to interception or unauthorized access. furthermore, each point - to - point connection represents a potential entry point for cyber threats. if one connection is compromised, it can expose the entire network to security breaches. this makes it crucial to implement robust security measures at each endpoint and ensure regular monitoring and updates to prevent and detect any potential vulnerabilities. to mitigate these risks, organizations should consider implementing a comprehensive security strategy that includes strong encryption protocols, regular security audits, and network segmentation to minimize the impact of a potential breach. scalability issues in it infrastructure point - to - point connectivity can pose scalability challenges in it infrastructure. as connections increase, managing and scaling the infrastructure becomes more complex and time - consuming. adding new endpoints requires creating additional connections, resulting in a tangled web of connections that is difficult to manage and maintain. this can lead to increased operational costs and reduced efficiency. moreover, scaling point - to - point connections often requires manual configuration and intervention, which can be time - consuming and error - prone. this manual process can hinder the organization's\",\n",
       " \"regular security audits, and network segmentation to minimize the impact of a potential breach. scalability issues in it infrastructure point - to - point connectivity can pose scalability challenges in it infrastructure. as connections increase, managing and scaling the infrastructure becomes more complex and time - consuming. adding new endpoints requires creating additional connections, resulting in a tangled web of connections that is difficult to manage and maintain. this can lead to increased operational costs and reduced efficiency. moreover, scaling point - to - point connections often requires manual configuration and intervention, which can be time - consuming and error - prone. this manual process can hinder the organization's ability to quickly adapt to changing business requirements and integrate new applications or systems. organizations can leverage self - service integration solutions that automate connectivity to address these scalability issues. organizations can easily add or remove endpoints using self - service integration without manual intervention. this improves scalability, reduces the risk of errors, and accelerates the deployment of new connections. impact on network performance point - to - point connectivity can have a significant impact on network performance. as the number of connections increases, the network can become congested, leading to latency and reduced overall performance. each point - to - point connection consumes network resources, including bandwidth\",\n",
       " 's ability to quickly adapt to changing business requirements and integrate new applications or systems. organizations can leverage self - service integration solutions that automate connectivity to address these scalability issues. organizations can easily add or remove endpoints using self - service integration without manual intervention. this improves scalability, reduces the risk of errors, and accelerates the deployment of new connections. impact on network performance point - to - point connectivity can have a significant impact on network performance. as the number of connections increases, the network can become congested, leading to latency and reduced overall performance. each point - to - point connection consumes network resources, including bandwidth and processing power. this can result in network bottlenecks and decreased performance for other applications and users. furthermore, managing and monitoring multiple point - to - point connections can be resource - intensive and require dedicated network administrators. this can divert resources from other critical tasks and impact overall network efficiency. organizations should consider implementing alternative connectivity approaches, such as using a hub - and - spoke or mesh network topology to mitigate the impact on network performance. these approaches provide centralized management and better utilization of network resources, resulting in improved performance and scalability. benefits of leveraging self - service integration to overcome p2p obstacles point - to - point',\n",
       " 'bandwidth and processing power. this can result in network bottlenecks and decreased performance for other applications and users. furthermore, managing and monitoring multiple point - to - point connections can be resource - intensive and require dedicated network administrators. this can divert resources from other critical tasks and impact overall network efficiency. organizations should consider implementing alternative connectivity approaches, such as using a hub - and - spoke or mesh network topology to mitigate the impact on network performance. these approaches provide centralized management and better utilization of network resources, resulting in improved performance and scalability. benefits of leveraging self - service integration to overcome p2p obstacles point - to - point connectivity in it infrastructure can present several challenges, but leveraging self - service integration can help overcome these obstacles. by using self - service integration, organizations can streamline their connectivity processes and reduce the need for manual intervention. this saves time and resources and minimizes the risk of errors that can occur during manual configuration. self - service integration also enables better collaboration between different systems and applications. it allows seamless data exchange and synchronization, facilitating efficient communication between various it infrastructure components. this can improve overall productivity and enhance the user experience. additionally, self - service integration provides greater flexibility and agility in adapting to changing business requirements',\n",
       " \"point connectivity in it infrastructure can present several challenges, but leveraging self - service integration can help overcome these obstacles. by using self - service integration, organizations can streamline their connectivity processes and reduce the need for manual intervention. this saves time and resources and minimizes the risk of errors that can occur during manual configuration. self - service integration also enables better collaboration between different systems and applications. it allows seamless data exchange and synchronization, facilitating efficient communication between various it infrastructure components. this can improve overall productivity and enhance the user experience. additionally, self - service integration provides greater flexibility and agility in adapting to changing business requirements. it allows organizations to quickly integrate new applications or systems into their infrastructure without disrupting the entire network. this scalability and adaptability can greatly benefit organizations in today's fast - paced and dynamic business environment. today, the integration center is working very well in describing its application networks and building integrations, and it has many different parties that need data. however, one department in the company needs data more than anyone else - the bi department. the business intelligence department might sit with a data lake and a bi function. many are still working with analytical bi, where one might look at the last week's data, the last\",\n",
       " \"requirements. it allows organizations to quickly integrate new applications or systems into their infrastructure without disrupting the entire network. this scalability and adaptability can greatly benefit organizations in today's fast - paced and dynamic business environment. today, the integration center is working very well in describing its application networks and building integrations, and it has many different parties that need data. however, one department in the company needs data more than anyone else - the bi department. the business intelligence department might sit with a data lake and a bi function. many are still working with analytical bi, where one might look at the last week's data, the last month's data, and, in the best case, the last day's data. very few are working with operational bi yet, meaning they don't understand what is happening in the reports. how do we act in our business right now to meet certain needs? the integration centers play an extremely important role here. the bi team needs more data than any other team, and they need current data. therefore, it is necessary to supplement batch data loading with streaming data. new solutions are needed, for example, based on kafka instead of using etl tools. what is particularly interesting now is that the bi team is\",\n",
       " \"last month's data, and, in the best case, the last day's data. very few are working with operational bi yet, meaning they don't understand what is happening in the reports. how do we act in our business right now to meet certain needs? the integration centers play an extremely important role here. the bi team needs more data than any other team, and they need current data. therefore, it is necessary to supplement batch data loading with streaming data. new solutions are needed, for example, based on kafka instead of using etl tools. what is particularly interesting now is that the bi team is not only working with business intelligence ; they have started working with artificial intelligence, or ai. an extremely important aspect of this is the availability of data. you can only train a good model if you have data to train on. the companies that have not yet started collecting data in their data lakes will not have any data to train on. if we see the bi development continue as quickly as it is now, including the specific area of ai, there will be a watershed within maybe a year. the companies that have data to apply their ai on and those that do not have data to apply their ai on. and this will then be decided\",\n",
       " \"is not only working with business intelligence ; they have started working with artificial intelligence, or ai. an extremely important aspect of this is the availability of data. you can only train a good model if you have data to train on. the companies that have not yet started collecting data in their data lakes will not have any data to train on. if we see the bi development continue as quickly as it is now, including the specific area of ai, there will be a watershed within maybe a year. the companies that have data to apply their ai on and those that do not have data to apply their ai on. and this will then be decided by not how good a bi product you have but how good an integration product you have. how much data have you transported from your application network to your bi team? what is required of the bi team for this to happen? they need to understand how the company's application network looks. where is data located? what data are we transporting today? what data should we be transporting today to be able to do better bi and even ai tomorrow? bi and integration managers need to talk more to each other. this is about today's challenge of achieving operational bi and tomorrow's challenge of getting data to train on for the coming ai\",\n",
       " \"decided by not how good a bi product you have but how good an integration product you have. how much data have you transported from your application network to your bi team? what is required of the bi team for this to happen? they need to understand how the company's application network looks. where is data located? what data are we transporting today? what data should we be transporting today to be able to do better bi and even ai tomorrow? bi and integration managers need to talk more to each other. this is about today's challenge of achieving operational bi and tomorrow's challenge of getting data to train on for the coming ai function. otherwise, there will be no cognitive systems in the company. no data - no cognition. in a groundbreaking move, entiros unveiled starlify, a cutting - edge solution designed to address public sector organizations'pervasive challenges in managing their system landscapes. the lack of clarity and understanding of the current state often hinders effective decision - making, leading to increased costs and inefficiencies in it projects. starlify launches in swedish cloud service endorsed by skr entiros proudly announces the official launch of starlify in a swedish cloud service approved by skr ( swedish association of local authorities and\",\n",
       " \"ai function. otherwise, there will be no cognitive systems in the company. no data - no cognition. in a groundbreaking move, entiros unveiled starlify, a cutting - edge solution designed to address public sector organizations'pervasive challenges in managing their system landscapes. the lack of clarity and understanding of the current state often hinders effective decision - making, leading to increased costs and inefficiencies in it projects. starlify launches in swedish cloud service endorsed by skr entiros proudly announces the official launch of starlify in a swedish cloud service approved by skr ( swedish association of local authorities and regions ). this strategic move positions starlify as a secure and reliable solution for public sector entities in sweden. revolutionizing system overview and efficiency public sector organizations have started using the tool to find dependencies and improve efficiency. from before, among others, volvo cars, with one of sweden's largest application networks, uses starlify. entiros is now looking forward to further collaborations to support sweden's municipalities and regions in their challenges. addressing critical issues in public sector it the struggle to understand the system's current state in the public sector often results in ineffective strategies, resource wastage, and budget breaches.\",\n",
       " \"and regions ). this strategic move positions starlify as a secure and reliable solution for public sector entities in sweden. revolutionizing system overview and efficiency public sector organizations have started using the tool to find dependencies and improve efficiency. from before, among others, volvo cars, with one of sweden's largest application networks, uses starlify. entiros is now looking forward to further collaborations to support sweden's municipalities and regions in their challenges. addressing critical issues in public sector it the struggle to understand the system's current state in the public sector often results in ineffective strategies, resource wastage, and budget breaches. in addition, it becomes difficult to see and follow the benefits and effects of the projects. as an example, over sek 3 billion ( 2018 ), out of a total of sek 9. 4 billion, went to projects where the benefit or effect could not be followed after the end of the project. this ultimately impacts the ability to deliver high - quality services to the community. starlify aims to rectify this by providing a user - friendly tool to collect and create comprehensive overviews of systems and applications. key features and user benefits starlify facilitates quick mapping and visualization, enabling efficient planning before implementing changes. this\",\n",
       " '. in addition, it becomes difficult to see and follow the benefits and effects of the projects. as an example, over sek 3 billion ( 2018 ), out of a total of sek 9. 4 billion, went to projects where the benefit or effect could not be followed after the end of the project. this ultimately impacts the ability to deliver high - quality services to the community. starlify aims to rectify this by providing a user - friendly tool to collect and create comprehensive overviews of systems and applications. key features and user benefits starlify facilitates quick mapping and visualization, enabling efficient planning before implementing changes. this occurs within a secure environment that fosters collaboration between internal and external suppliers. crucially, the tool supports the creation of efficient management processes for sensitive information, ensuring compliance with regulations. anders holmstrand, product manager at starlify, comments : \" starlify empowers public sector organizations to take control of their system landscape and effectively manage changes with insight and security. our presence at the solutions for public sector trade show in january offers an excellent opportunity for visitors to learn more about our tool and discuss their challenges in it projects and change work. \" what is psd2? psd2, short for the second payment',\n",
       " 'this occurs within a secure environment that fosters collaboration between internal and external suppliers. crucially, the tool supports the creation of efficient management processes for sensitive information, ensuring compliance with regulations. anders holmstrand, product manager at starlify, comments : \" starlify empowers public sector organizations to take control of their system landscape and effectively manage changes with insight and security. our presence at the solutions for public sector trade show in january offers an excellent opportunity for visitors to learn more about our tool and discuss their challenges in it projects and change work. \" what is psd2? psd2, short for the second payment services directive, aims to revolutionize payment services. this regulation opens consumer bank accounts to third - party providers ( tpps ). unlocking vast amounts of data banks hold. it levels the playing field and bridges the gap between traditional banks, fintech, and big techs. it represents a shift in the european banking sector, moving it toward the era of open banking. how it affects banks the impact of psd2 on banks is monumental. this regulation grants third - party providers access to bank data. however, its implementation faced hurdles, experiencing delays and partial enforcement in different countries. two of the core aspects of psd2',\n",
       " \"payment services directive, aims to revolutionize payment services. this regulation opens consumer bank accounts to third - party providers ( tpps ). unlocking vast amounts of data banks hold. it levels the playing field and bridges the gap between traditional banks, fintech, and big techs. it represents a shift in the european banking sector, moving it toward the era of open banking. how it affects banks the impact of psd2 on banks is monumental. this regulation grants third - party providers access to bank data. however, its implementation faced hurdles, experiencing delays and partial enforcement in different countries. two of the core aspects of psd2 faced challenges in full deployment. these were strong customer authentication ( sca ) and standardized apis for account interfaces. the banks navigated through these complexities while trying to adapt to the regulatory landscape. why must banks adapt integration landscapes for psd2 compliance? the challenge for banks is twofold : achieving compliance while maintaining technological readiness. this requires meeting defined standards and preparing for potential revisions and expansions. banks must balance regulatory compliance demands with the agility needed to adapt. the adaptation process is a continuous journey that's necessary to secure compliance. this ongoing adjustment ensures that banks are ready for upcoming regulatory changes. integration discovery for ps\",\n",
       " \"##2 faced challenges in full deployment. these were strong customer authentication ( sca ) and standardized apis for account interfaces. the banks navigated through these complexities while trying to adapt to the regulatory landscape. why must banks adapt integration landscapes for psd2 compliance? the challenge for banks is twofold : achieving compliance while maintaining technological readiness. this requires meeting defined standards and preparing for potential revisions and expansions. banks must balance regulatory compliance demands with the agility needed to adapt. the adaptation process is a continuous journey that's necessary to secure compliance. this ongoing adjustment ensures that banks are ready for upcoming regulatory changes. integration discovery for psd2 compliance implementing an integration solution for psd2 compliance requires a strategic approach. an approach that guarantees efficiency and rapid adaptability. banks and other financial institutions must chart a straightforward roadmap to navigate both swiftly and effectively. otherwise, it's unlikely that the psd2 compliance will succeed. how do they do it? integration discovery. 1. strategic alignment and objectives setting : prioritize aligning your integration strategy with the overarching goals of psd2 compliance. set clear objectives and outline the wanted outcomes from the integration process. establish a dedicated team with it, compliance, security, and business strategy experts to\",\n",
       " \"psd2 compliance implementing an integration solution for psd2 compliance requires a strategic approach. an approach that guarantees efficiency and rapid adaptability. banks and other financial institutions must chart a straightforward roadmap to navigate both swiftly and effectively. otherwise, it's unlikely that the psd2 compliance will succeed. how do they do it? integration discovery. 1. strategic alignment and objectives setting : prioritize aligning your integration strategy with the overarching goals of psd2 compliance. set clear objectives and outline the wanted outcomes from the integration process. establish a dedicated team with it, compliance, security, and business strategy experts to work with the initiative. 2. comprehensive landscape assessment : initiate with a complete assessment of your current integration landscape. engage stakeholders, it teams, and relevant departments to understand your current state. that involves existing systems, apis, data architecture, security protocols, and integrations. document and analyze the current state and potential areas for improvement. har kan starlify hjalpa till. 3. technology stack evaluation : evaluate your technology stack against psd2 compliance requirements. identify any gaps between the existing technology stack and the compliance prerequisites. 4. collaboration and partnership exploration : foster collaborations and partnerships within the industry and engage\",\n",
       " 'to work with the initiative. 2. comprehensive landscape assessment : initiate with a complete assessment of your current integration landscape. engage stakeholders, it teams, and relevant departments to understand your current state. that involves existing systems, apis, data architecture, security protocols, and integrations. document and analyze the current state and potential areas for improvement. har kan starlify hjalpa till. 3. technology stack evaluation : evaluate your technology stack against psd2 compliance requirements. identify any gaps between the existing technology stack and the compliance prerequisites. 4. collaboration and partnership exploration : foster collaborations and partnerships within the industry and engage with both third - party providers and internal experts to leverage their expertise and experiences. explore collaborations that can expedite the implementation of necessary changes and adaptations to meet compliance requirements. and, of course, collaborations that can help you get an overview of your current state. 5. creation of a rapid deployment plan : utilize the insights from the assessment and analysis phases to devise a rapid deployment plan. prioritize actions that address critical gaps and compliance necessities. craft a phased approach, allowing incremental improvements while aligning with regulatory timelines. 6. implementation and continuous iteration : implement the planned changes and adaptations in a phased',\n",
       " \"engage with both third - party providers and internal experts to leverage their expertise and experiences. explore collaborations that can expedite the implementation of necessary changes and adaptations to meet compliance requirements. and, of course, collaborations that can help you get an overview of your current state. 5. creation of a rapid deployment plan : utilize the insights from the assessment and analysis phases to devise a rapid deployment plan. prioritize actions that address critical gaps and compliance necessities. craft a phased approach, allowing incremental improvements while aligning with regulatory timelines. 6. implementation and continuous iteration : implement the planned changes and adaptations in a phased manner. continuously refine the integration discovery process based on real - time insights and developments. review progress and adapt the approach to ensure swift compliance without compromising quality. 7. documentation and knowledge transfer : document every step, change, and implementation made throughout the integration discovery process. make sure your organization and relevant stakeholders understand the insights gained and the solutions implemented. this can be achieved with a solution that visualizes and shows dependencies across your application network. are you tired of hearing about rest, graphql, and asynch apis without really understanding what they mean? well, don't worry ; you're not alone\",\n",
       " 'phased manner. continuously refine the integration discovery process based on real - time insights and developments. review progress and adapt the approach to ensure swift compliance without compromising quality. 7. documentation and knowledge transfer : document every step, change, and implementation made throughout the integration discovery process. make sure your organization and relevant stakeholders understand the insights gained and the solutions implemented. this can be achieved with a solution that visualizes and shows dependencies across your application network. are you tired of hearing about rest, graphql, and asynch apis without really understanding what they mean? well, don\\'t worry ; you\\'re not alone. in the world of application development, there are a lot of different technologies to choose from, and it can be overwhelming to figure out which one is right for your project. so let\\'s break it down. restful apis ; best for simple crud operations first up, we have rest. now, i know what you\\'re thinking, \" representational state transfer, that sounds fancy and confusing! \" but it\\'s really not that complicated. restful apis use http requests to post ( create ), put ( update ), get ( read ), and delete data, and they\\'re based on',\n",
       " 'alone. in the world of application development, there are a lot of different technologies to choose from, and it can be overwhelming to figure out which one is right for your project. so let\\'s break it down. restful apis ; best for simple crud operations first up, we have rest. now, i know what you\\'re thinking, \" representational state transfer, that sounds fancy and confusing! \" but it\\'s really not that complicated. restful apis use http requests to post ( create ), put ( update ), get ( read ), and delete data, and they\\'re based on familiar concepts like urls and http. they are also often used with web services that return data in json or xml format. graphql ; best for complex data requirements next, we have graphql, which is a newer technology developed by facebook. graphql is a query language that allows developers to request exactly the data they need rather than getting a fixed set of data from an endpoint. it\\'s kind of like going to a restaurant and being able to order exactly what you want rather than being forced to order a pre - set meal. this can make it more efficient to work with, especially for more complex',\n",
       " \"on familiar concepts like urls and http. they are also often used with web services that return data in json or xml format. graphql ; best for complex data requirements next, we have graphql, which is a newer technology developed by facebook. graphql is a query language that allows developers to request exactly the data they need rather than getting a fixed set of data from an endpoint. it's kind of like going to a restaurant and being able to order exactly what you want rather than being forced to order a pre - set meal. this can make it more efficient to work with, especially for more complex data requirements. in addition, graphql also provides a type system that allows for more efficient validation and better documentation of the api. asynch apis ; best for real - time applications last but not least, we have asynch apis, which are becoming increasingly popular recently. as the name suggests, these apis work asynchronously, which means that clients can make requests and receive responses as soon as they're available rather than having to wait for a response from the server. this can be really useful for real - time applications like chat or gaming. choosing the api technology that best fits your needs\",\n",
       " \"complex data requirements. in addition, graphql also provides a type system that allows for more efficient validation and better documentation of the api. asynch apis ; best for real - time applications last but not least, we have asynch apis, which are becoming increasingly popular recently. as the name suggests, these apis work asynchronously, which means that clients can make requests and receive responses as soon as they're available rather than having to wait for a response from the server. this can be really useful for real - time applications like chat or gaming. choosing the api technology that best fits your needs so which one is right for your project? well, it depends on your specific needs. if you're working on a simple crud ( create, read, update, delete ) application, restful apis are probably a good choice. if you need more complex data requirements, graphql might be a better fit. and if you're working on a real - time application, asynch apis are definitely worth considering. seek guidance from an integration company if you need help at the end of the day, the most important thing is to evaluate the requirements of your project and choose the technology that best fits your\",\n",
       " \"popular recently. as the name suggests, these apis work asynchronously, which means that clients can make requests and receive responses as soon as they're available rather than having to wait for a response from the server. this can be really useful for real - time applications like chat or gaming. choosing the api technology that best fits your needs so which one is right for your project? well, it depends on your specific needs. if you're working on a simple crud ( create, read, update, delete ) application, restful apis are probably a good choice. if you need more complex data requirements, graphql might be a better fit. and if you're working on a real - time application, asynch apis are definitely worth considering. seek guidance from an integration company if you need help at the end of the day, the most important thing is to evaluate the requirements of your project and choose the technology that best fits your needs. and if you're still feeling overwhelmed, don't hesitate to reach out to an integration company ( like entiros integrations [UNK] ) for guidance. they'll be able to help you navigate the world of apis and choose the right one for your project.\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dimension of the embeddings\n",
    "dimension = vectors.shape[1]\n",
    "\n",
    "# Create a FAISS index\n",
    "index_DB = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add vectors to the index\n",
    "index_DB.add(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 512 tokens, input is 22 tokens\n"
     ]
    }
   ],
   "source": [
    "# Embed a query text\n",
    "query_text = \"What are the security risks and vulnerabilities associated with point-to-point connectivity in IT infrastructure?\"\n",
    "query_vector, _ = embed_text(query_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 384)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What are the security risks and vulnerabilities associated with point-to-point connectivity in IT infrastructure?\n",
      "Top 3 most similar texts:\n",
      "1. 5 (Distance: 7.666614532470703) to - point connections'complexity and maintenance overhead. monitoring and troubleshooting each connection individually becomes challenging as the number of connections increases. another limitation is the lack of scalability. point - to - point connections are usually established one - to - one, meaning adding new endpoints requires creating additional connections. this can lead to a tangled web of connections that becomes difficult to manage and scale as the infrastructure grows. furthermore, point - to - point connectivity can result in vendor lock - in. each connection is typically configured with specific protocols and standards, making switching vendors or integrating new technologies challenging. this lack of interoperability can limit the organization's ability to adopt new solutions or take advantage of emerging technologies. lastly, point - to - point connections can also introduce security risks. each connection represents a potential entry point for cyber threats, and securing each connection individually can be a complex task. additionally, point - to - point connections may not provide the security and encryption required to protect sensitive data. security risks and vulnerabilities point - to - point connectivity in it infrastructure can introduce various security risks and vulnerabilities. one of the main concerns is the lack of centralized security controls. with multiple point - to - point connections, enforcing consistent security policies across the entire \n",
      "2. 8 (Distance: 11.298086166381836) regular security audits, and network segmentation to minimize the impact of a potential breach. scalability issues in it infrastructure point - to - point connectivity can pose scalability challenges in it infrastructure. as connections increase, managing and scaling the infrastructure becomes more complex and time - consuming. adding new endpoints requires creating additional connections, resulting in a tangled web of connections that is difficult to manage and maintain. this can lead to increased operational costs and reduced efficiency. moreover, scaling point - to - point connections often requires manual configuration and intervention, which can be time - consuming and error - prone. this manual process can hinder the organization's ability to quickly adapt to changing business requirements and integrate new applications or systems. organizations can leverage self - service integration solutions that automate connectivity to address these scalability issues. organizations can easily add or remove endpoints using self - service integration without manual intervention. this improves scalability, reduces the risk of errors, and accelerates the deployment of new connections. impact on network performance point - to - point connectivity can have a significant impact on network performance. as the number of connections increases, the network can become congested, leading to latency and reduced overall performance. each point - to - point connection consumes network resources, including bandwidth \n",
      "3. 6 (Distance: 11.876522064208984) organization's ability to adopt new solutions or take advantage of emerging technologies. lastly, point - to - point connections can also introduce security risks. each connection represents a potential entry point for cyber threats, and securing each connection individually can be a complex task. additionally, point - to - point connections may not provide the security and encryption required to protect sensitive data. security risks and vulnerabilities point - to - point connectivity in it infrastructure can introduce various security risks and vulnerabilities. one of the main concerns is the lack of centralized security controls. with multiple point - to - point connections, enforcing consistent security policies across the entire network becomes challenging. another risk is the potential exposure of sensitive data during transit. point - to - point connections may not provide adequate encryption or secure transmission protocols, leaving data vulnerable to interception or unauthorized access. furthermore, each point - to - point connection represents a potential entry point for cyber threats. if one connection is compromised, it can expose the entire network to security breaches. this makes it crucial to implement robust security measures at each endpoint and ensure regular monitoring and updates to prevent and detect any potential vulnerabilities. to mitigate these risks, organizations should consider implementing a comprehensive security strategy that includes strong encryption protocols, regular \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Perform a similarity search\n",
    "k = 3  # Number of nearest neighbors to retrieve\n",
    "distances, indices = index_DB.search(query_vector, k)\n",
    "\n",
    "# Retrieve the most similar texts\n",
    "print(\"Query:\", query_text)\n",
    "print(\"Top 3 most similar texts:\")\n",
    "for i, idx in enumerate(indices[0]):\n",
    "    print(f\"{i+1}. {idx} (Distance: {distances[0][i]}) {lookup[idx]} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 79 tokens\n",
      "\n",
      "User: whome do i speak to?\n",
      "Assistent: You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\n",
      "User: What are the security risks and vulnerabilities associated with point-to-point connectivity in IT infrastructure?\n",
      "Assistent: 1. Security risks: Point-to-point connectivity can be vulnerable to various security threats, including: 1.1 Unauthorized access: If an attacker gains unauthorized access to the point-to-point connection, they can potentially intercept sensitive data or disrupt the network. 1.2 Malware: Point-to-point connectivity can be a target for malware attacks, such as viruses, worms, or Trojans, which can compromise the security of the entire network. 1.\n"
     ]
    }
   ],
   "source": [
    "input_text = f\"\"\"\n",
    "User: whome do i speak to?\n",
    "Assistent: You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\n",
    "User: {query_text}\n",
    "Assistent: \"\"\"\n",
    "\n",
    "output = generated_text(input_text)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 353 tokens\n",
      "\n",
      "User: whome do i speak to?\n",
      "Assistent: You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\n",
      "User: What do you know about Entiros?\n",
      "Assistent: This is what i found on the wab page of Entiros: to - point connections'complexity and maintenance overhead. monitoring and troubleshooting each connection individually becomes challenging as the number of connections increases. another limitation is the lack of scalability. point - to - point connections are usually established one - to - one, meaning adding new endpoints requires creating additional connections. this can lead to a tangled web of connections that becomes difficult to manage and scale as the infrastructure grows. furthermore, point - to - point connectivity can result in vendor lock - in. each connection is typically configured with specific protocols and standards, making switching vendors or integrating new technologies challenging. this lack of interoperability can limit the organization's ability to adopt new solutions or take advantage of emerging technologies. lastly, point - to - point connections can also introduce security risks. each connection represents a potential entry point for cyber threats, and securing each connection individually can be a complex task. additionally, point - to - point connections may not provide the security and encryption required to protect sensitive data. security risks and vulnerabilities point - to - point connectivity in it infrastructure can introduce various security risks and vulnerabilities. one of the main concerns is the lack of centralized security controls. with multiple point - to - point connections, enforcing consistent security policies across the entire\n",
      "User: What are the security risks and vulnerabilities associated with point-to-point connectivity in IT infrastructure?\n",
      "Assistent: 1. Lack of centralized security controls: point-to-point connectivity in IT infrastructure can introduce various security risks and vulnerabilities. One of the main concerns is the lack of centralized security controls. With multiple point-to-point connections, enforcing consistent security policies across the entire IT infrastructure can be challenging. This can lead to a lack of centralized security controls, making it difficult to ensure that all endpoints are secure and protected from potential threats. Additionally, the use of point-to-point connectivity can make\n"
     ]
    }
   ],
   "source": [
    "input_text = f\"\"\"\n",
    "User: whome do i speak to?\n",
    "Assistent: You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\n",
    "User: What do you know about Entiros?\n",
    "Assistent: This is what i found on the wab page of Entiros: {lookup[indices[0][0]]}\n",
    "User: {query_text}\n",
    "Assistent: \"\"\"\n",
    "\n",
    "output = generated_text(input_text)\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 384)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruct Models\n",
    "Instruct models are a type of Large Language Model (LLM) specifically trained to follow instructions given in natural language. Unlike general-purpose LLMs, which are trained to predict the next word in a sequence, instruct models are fine-tuned to respond to user prompts by performing specific tasks or answering questions in a way that aligns with the given instructions.\n",
    "\n",
    "* **Instruction-Following:** These models excel at understanding and executing tasks described in plain language prompts.\n",
    "* **Fine-Tuning:** Instruct models are typically fine-tuned on datasets that include pairs of instructions and the desired outputs, making them better at adhering to the user's intent.\n",
    "* **Versatility:** They can handle a wide range of tasks, such as answering questions, generating summaries, providing step-by-step instructions, writing code, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208bd5e9d9a14eaea63f7400a2ac87da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.59k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acb72b2a2594f44bf7a4f41c27b0062",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a93f8c2f012240f2b153958339fac977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb17176bceb14e5fb36827261e50ea02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "613b888c9cb94918a240503a1e9c0450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/565 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8a53d0b56d421e83f4655938718d0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/724 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54179a3404bf474e8495eef03791a8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/724M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbb3582a7279432c833a1595f7dfa9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "checkpoint = \"HuggingFaceTB/SmolLM-360M-Instruct\"\n",
    "\n",
    "inst_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inst_model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "inst_config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 63 tokens\n",
      "<|im_start|>assistent\n",
      "You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you give me a short summary of what Entiros does?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Entiros is a popular online chat platform that provides a range of features and services to help users communicate with each other. Here's a brief summary:\n",
      "\n",
      "**What is Entiros?**\n",
      "\n",
      "Entiros is a social media platform that allows users to create profiles, connect with others, and engage in conversations. It's designed to be a user-friendly and feature-rich environment for people to interact with each other.\n",
      "\n",
      "**Features:**\n",
      "\n",
      "1.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [{\"role\": \"assistent\", \"content\": \"You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Can you give me a short summary of what Entiros does?\"}]\n",
    "\n",
    "input_text=inst_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "output = generated_text(input_text, tokenizer=inst_tokenizer, model=inst_model, config=inst_config)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 383 tokens\n",
      "<|im_start|>assistent\n",
      "You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.<|im_end|>\n",
      "<|im_start|>user\n",
      "What do you know about Entiros?<|im_end|>\n",
      "<|im_start|>assistent\n",
      "This is what i found on the wab page of Entiros: Better ways to build integrations. Data-informed connectivity. Our approach combines data analysis with experience to make data-informed network decisions, ensuring relevance and effectiveness in every integration.. Data-driven connectivity. Customer case. Find out how adopting technological advancements simplifies processes, boosts expansion, and gears up for upcoming obstacles in the constantly changing automotive sector.. \"Lorem Ipsum text asd asd asd asd asd asd asd asd asd asd asd asd asd asd asd \". Företag ABC. John Johnsson. \"Lorem Ipsum\". Företag ABC. John Johnson. Let's connect. About us. Entiros is a specialized integration company full of integrators, architects, and talented people dedicated to connecting the world.. We offer better ways to build integrations by addressing fundamental industry challenges - both for large companies and people in the industry. We support you through the entire chain, from integration strategy to implementation, management, and operations.. Other services we offer. Starlify. Build better integrations by collaboration and insight. Starlify is a productivity tool for creating smarter connectivity in any IT landscape.. Certified Integrator. Partnerships. Solutions. Event streams\n",
      "Our network. Our network. About us. Integration blog. Connect. An Entiros company. © 2024 Entiros. All rights reserved.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you give me a short summary of what Entiros does?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Entiros is a company that provides a range of services to help businesses connect with customers, partners, and suppliers more effectively. Here's a brief summary of what they do:\n",
      "\n",
      "**Services:**\n",
      "\n",
      "1. **Integration**: Entiros helps businesses integrate their systems, applications, and data from various sources to create a seamless and efficient customer experience.\n",
      "2. **Customer Engagement**: They provide solutions to improve customer engagement, such as providing personalized experiences, building\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [{\"role\": \"assistent\", \"content\": \"You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\"},\n",
    "            {\"role\": \"user\", \"content\": \"What do you know about Entiros?\"}, \n",
    "            {\"role\": \"assistent\", \"content\": f\"This is what i found on the wab page of Entiros: {web_info}\"},\n",
    "            {\"role\": \"user\", \"content\": \"Can you give me a short summary of what Entiros does?\"}]\n",
    "\n",
    "input_text=inst_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "output = generated_text(input_text, tokenizer=inst_tokenizer, model=inst_model, config=inst_config, max_new_tokens=100)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 68 tokens\n",
      "<|im_start|>assistent\n",
      "You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.<|im_end|>\n",
      "<|im_start|>user\n",
      "What are the security risks and vulnerabilities associated with point-to-point connectivity in IT infrastructure?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Point-to-point connectivity in IT infrastructure can pose significant security risks and vulnerabilities. Here are some of the common security risks and vulnerabilities associated with point-to-point connectivity:\n",
      "\n",
      "1. **Unsecured Devices**: Unsecured devices, such as laptops, desktops, or servers, can be easily hacked or compromised by malware, viruses, or other malicious software.\n",
      "2. **Insecure Data Storage**: Insecure data storage, such as unencrypted files\n"
     ]
    }
   ],
   "source": [
    "\n",
    "messages = [{\"role\": \"assistent\", \"content\": \"You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\"},\n",
    "            {\"role\": \"user\", \"content\": query_text}]\n",
    "input_text=inst_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "output = generated_text(input_text, tokenizer=inst_tokenizer, model=inst_model, config=inst_config, max_new_tokens=100)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 832 tokens\n",
      "<|im_start|>assistent\n",
      "You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.<|im_end|>\n",
      "<|im_start|>user\n",
      "What do you know about Entiros?<|im_end|>\n",
      "<|im_start|>assistent\n",
      "This is what i found on the wab page of Entiros: to - point connections'complexity and maintenance overhead. monitoring and troubleshooting each connection individually becomes challenging as the number of connections increases. another limitation is the lack of scalability. point - to - point connections are usually established one - to - one, meaning adding new endpoints requires creating additional connections. this can lead to a tangled web of connections that becomes difficult to manage and scale as the infrastructure grows. furthermore, point - to - point connectivity can result in vendor lock - in. each connection is typically configured with specific protocols and standards, making switching vendors or integrating new technologies challenging. this lack of interoperability can limit the organization's ability to adopt new solutions or take advantage of emerging technologies. lastly, point - to - point connections can also introduce security risks. each connection represents a potential entry point for cyber threats, and securing each connection individually can be a complex task. additionally, point - to - point connections may not provide the security and encryption required to protect sensitive data. security risks and vulnerabilities point - to - point connectivity in it infrastructure can introduce various security risks and vulnerabilities. one of the main concerns is the lack of centralized security controls. with multiple point - to - point connections, enforcing consistent security policies across the entire\n",
      "regular security audits, and network segmentation to minimize the impact of a potential breach. scalability issues in it infrastructure point - to - point connectivity can pose scalability challenges in it infrastructure. as connections increase, managing and scaling the infrastructure becomes more complex and time - consuming. adding new endpoints requires creating additional connections, resulting in a tangled web of connections that is difficult to manage and maintain. this can lead to increased operational costs and reduced efficiency. moreover, scaling point - to - point connections often requires manual configuration and intervention, which can be time - consuming and error - prone. this manual process can hinder the organization's ability to quickly adapt to changing business requirements and integrate new applications or systems. organizations can leverage self - service integration solutions that automate connectivity to address these scalability issues. organizations can easily add or remove endpoints using self - service integration without manual intervention. this improves scalability, reduces the risk of errors, and accelerates the deployment of new connections. impact on network performance point - to - point connectivity can have a significant impact on network performance. as the number of connections increases, the network can become congested, leading to latency and reduced overall performance. each point - to - point connection consumes network resources, including bandwidth\n",
      "organization's ability to adopt new solutions or take advantage of emerging technologies. lastly, point - to - point connections can also introduce security risks. each connection represents a potential entry point for cyber threats, and securing each connection individually can be a complex task. additionally, point - to - point connections may not provide the security and encryption required to protect sensitive data. security risks and vulnerabilities point - to - point connectivity in it infrastructure can introduce various security risks and vulnerabilities. one of the main concerns is the lack of centralized security controls. with multiple point - to - point connections, enforcing consistent security policies across the entire network becomes challenging. another risk is the potential exposure of sensitive data during transit. point - to - point connections may not provide adequate encryption or secure transmission protocols, leaving data vulnerable to interception or unauthorized access. furthermore, each point - to - point connection represents a potential entry point for cyber threats. if one connection is compromised, it can expose the entire network to security breaches. this makes it crucial to implement robust security measures at each endpoint and ensure regular monitoring and updates to prevent and detect any potential vulnerabilities. to mitigate these risks, organizations should consider implementing a comprehensive security strategy that includes strong encryption protocols, regular<|im_end|>\n",
      "<|im_start|>user\n",
      "What are the security risks and vulnerabilities associated with point-to-point connectivity in IT infrastructure?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Point-to-point connectivity in IT infrastructure can introduce security risks and vulnerabilities, which can compromise the confidentiality, integrity, and availability of sensitive data and systems. Some of the common security risks and vulnerabilities associated with point-to-point connectivity include:\n",
      "\n",
      "1. **Unsecured Data**: Point-to-point connectivity can expose sensitive data, such as encryption keys, passwords, or confidential information, to unauthorized access.\n",
      "2. **Uncontrolled Data Transmission**: Without proper\n"
     ]
    }
   ],
   "source": [
    "context = \"\\n\".join([ lookup[idx] for idx in indices[0]])\n",
    "messages = [{\"role\": \"assistent\", \"content\": \"You are speaking to a chatbot. I am here to help you with any questions you may have. I will provide you with information and answer your questions to the best of my ability.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"What do you know about Entiros?\"},\n",
    "            {\"role\": \"assistent\", \"content\": f\"This is what i found on the wab page of Entiros: {context}\"},\n",
    "            {\"role\": \"user\", \"content\": query_text}]\n",
    "input_text=inst_tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "output = generated_text(input_text, tokenizer=inst_tokenizer, model=inst_model, config=inst_config, max_new_tokens=100)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try something completely different\n",
    "\n",
    "We would like to have some structured data because who wants to work with unstructured data can we use LLMs to help us with that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL dummy data\n",
    "\n",
    "import sqlite3\n",
    "import pathlib\n",
    "\n",
    "# remove the database file if it exists\n",
    "if pathlib.Path(\"data.db\").exists():\n",
    "    pathlib.Path(\"data.db\").unlink()\n",
    "\n",
    "conn = sqlite3.connect('data.db')\n",
    "\n",
    "c = conn.cursor()\n",
    "\n",
    "# Table for animals with columns: id, name, age, species\n",
    "c.execute('''CREATE TABLE animals\n",
    "             (id INTEGER PRIMARY KEY,\n",
    "              name TEXT NOT NULL,\n",
    "              age INTEGER NOT NULL,\n",
    "              species TEXT NOT NULL)''')\n",
    "\n",
    "# Table for people with columns: id, name and age\n",
    "c.execute('''CREATE TABLE people\n",
    "             (id INTEGER PRIMARY KEY,\n",
    "              name TEXT NOT NULL,\n",
    "              age INTEGER NOT NULL)''')\n",
    "\n",
    "# Table for ownership with columns: id, animal_id, person_id\n",
    "c.execute('''CREATE TABLE ownership\n",
    "                (id INTEGER PRIMARY KEY,\n",
    "                animal_id INTEGER NOT NULL,\n",
    "                person_id INTEGER NOT NULL)''')\n",
    "\n",
    "# Table for sickness with columns: id, name, description, treatment\n",
    "c.execute('''CREATE TABLE sickness\n",
    "                (id INTEGER PRIMARY KEY,\n",
    "                name TEXT NOT NULL,\n",
    "                description TEXT NOT NULL,\n",
    "                treatment TEXT NOT NULL)''')\n",
    "\n",
    "# Table for animal_sickness with columns: id, animal_id, sickness_id\n",
    "c.execute('''CREATE TABLE animal_sickness\n",
    "                (id INTEGER PRIMARY KEY,\n",
    "                animal_id INTEGER NOT NULL,\n",
    "                sickness_id INTEGER NOT NULL)''')\n",
    "\n",
    "# Insert data into the tables\n",
    "for animal in [(\"Fido\", 4, \"dog\"), (\"Whiskers\", 7, \"cat\"), (\"Fluffy\", 2, \"rabbit\")]:\n",
    "    c.execute(\"INSERT INTO animals (name, age, species) VALUES (?, ?, ?)\", animal)\n",
    "\n",
    "for person in [(\"Alice\", 25), (\"Bob\", 30), (\"Charlie\", 35)]:\n",
    "    c.execute(\"INSERT INTO people (name, age) VALUES (?, ?)\", person)\n",
    "\n",
    "for ownership in [(1, 1), (2, 2), (3, 3), (2, 1)]:\n",
    "    c.execute(\"INSERT INTO ownership (animal_id, person_id) VALUES (?, ?)\", ownership)\n",
    "\n",
    "for sickness in [(\"Flu\", \"A common viral infection that can be deadly\", \"Rest and drink plenty of fluids\"),\n",
    "                 (\"Broken leg\", \"A fracture in the bone\", \"Surgery and a cast\"),\n",
    "                 (\"Ear infection\", \"An infection in the ear\", \"Antibiotics\")]:\n",
    "    c.execute(\"INSERT INTO sickness (name, description, treatment) VALUES (?, ?, ?)\", sickness)\n",
    "\n",
    "for animal_sickness in [(1, 1), (2, 2), (3, 3), (1, 2), (1, 3)]:\n",
    "    c.execute(\"INSERT INTO animal_sickness (animal_id, sickness_id) VALUES (?, ?)\", animal_sickness)\n",
    "\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table: animals, Columns: ['id', 'name', 'age', 'species']\n",
      "Table: people, Columns: ['id', 'name', 'age']\n",
      "Table: ownership, Columns: ['id', 'animal_id', 'person_id']\n",
      "Table: sickness, Columns: ['id', 'name', 'description', 'treatment']\n",
      "Table: animal_sickness, Columns: ['id', 'animal_id', 'sickness_id']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "c.execute(query)\n",
    "tables = c.fetchall()\n",
    "tables = [table[0] for table in tables]\n",
    "info = []\n",
    "\n",
    "for table in tables:\n",
    "    # find meta data for the table\n",
    "    query = f\"PRAGMA table_info({table})\"\n",
    "    c.execute(query)\n",
    "    columns = c.fetchall()\n",
    "    columns = [column[1] for column in columns]\n",
    "    info.append(f\"Table: {table}, Columns: {columns}\")\n",
    "\n",
    "info = \"\\n\".join(info)\n",
    "print(info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 349 tokens\n",
      "<|im_start|>assistent\n",
      "You are an expert SQL chatbot. Generate ONLY SQL queries what answer user requests. you do not provide any other information.<|im_end|>\n",
      "<|im_start|>user\n",
      "I have a SQLite database with Table: animals, Columns: ['id', 'name', 'age', 'species']\n",
      "Table: people, Columns: ['id', 'name', 'age']\n",
      "Table: ownership, Columns: ['id', 'animal_id', 'person_id']\n",
      "Table: sickness, Columns: ['id', 'name', 'description', 'treatment']\n",
      "Table: animal_sickness, Columns: ['id', 'animal_id', 'sickness_id']. Can you give me the SQL query to select the youngest animal? and there owners? <|im_end|>\n",
      "<|im_start|>assistent\n",
      "SELECT animals.name, people.name FROM animals JOIN ownership ON animals.id = ownership.animal_id JOIN people ON ownership.person_id = people.id WHERE animals.age = (SELECT MIN(age) FROM animals)<|im_end|>\n",
      "<|im_start|>user\n",
      "I have a SQLite database with Table: animals, Columns: ['id', 'name', 'age', 'species']\n",
      "Table: people, Columns: ['id', 'name', 'age']\n",
      "Table: ownership, Columns: ['id', 'animal_id', 'person_id']\n",
      "Table: sickness, Columns: ['id', 'name', 'description', 'treatment']\n",
      "Table: animal_sickness, Columns: ['id', 'animal_id', 'sickness_id']. Can you give me the SQL query to select the person with the most animals?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "SELECT person_id, COUNT(*) AS total FROM people JOIN animals ON animals.id = people.animal_id JOIN ownership ON people.person_id = animals.id JOIN sickness ON animals.id = people.sickness_id GROUP BY person_id\n",
      "\n",
      "You can run the query and get the result as a list of tuples of the form `(person_id, total)`.\n",
      "\n",
      "For example, if we want to\n"
     ]
    }
   ],
   "source": [
    "message = [{\"role\": \"assistent\", \"content\": \"You are an expert SQL chatbot. Generate ONLY SQL queries what answer user requests. you do not provide any other information.\"},\n",
    "           {\"role\": \"user\", \"content\": f\"I have a SQLite database with {info}. Can you give me the SQL query to select the youngest animal? and there owners? \"}, \n",
    "           {\"role\": \"assistent\", \"content\": \"SELECT animals.name, people.name FROM animals JOIN ownership ON animals.id = ownership.animal_id JOIN people ON ownership.person_id = people.id WHERE animals.age = (SELECT MIN(age) FROM animals)\"},\n",
    "        #    {\"role\": \"user\", \"content\": f\"I have a SQLite database with {info}. Can you give me the SQL query to select the animals that have the flu?\"},\n",
    "        #    {\"role\": \"assistent\", \"content\": \"SELECT animals.name FROM animals JOIN animal_sickness ON animals.id = animal_sickness.animal_id JOIN sickness ON animal_sickness.sickness_id = sickness.id WHERE sickness.name = 'Flu'\"},\n",
    "           {\"role\": \"user\", \"content\": f\"I have a SQLite database with {info}. Can you give me the SQL query to select the person with the most animals?\"},\n",
    "\n",
    "           ]\n",
    "input_text=inst_tokenizer.apply_chat_template(message, tokenize=False)\n",
    "\n",
    "output = generated_text(input_text, tokenizer=inst_tokenizer, model=inst_model, config=inst_config, max_new_tokens=100)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "no such column: people.animal_id",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Query the database for the youngest animal\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT person_id, COUNT(*) AS total FROM people JOIN animals ON animals.id = people.animal_id JOIN ownership ON people.person_id = animals.id JOIN sickness ON animals.id = people.sickness_id GROUP BY person_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m youngest_animal \u001b[38;5;241m=\u001b[39m c\u001b[38;5;241m.\u001b[39mfetchone()\n\u001b[1;32m      4\u001b[0m youngest_animal\n",
      "\u001b[0;31mOperationalError\u001b[0m: no such column: people.animal_id"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query the database for the youngest animal\n",
    "c.execute(\"SELECT person_id, COUNT(*) AS total FROM people JOIN animals ON animals.id = people.animal_id JOIN ownership ON people.person_id = animals.id JOIN sickness ON animals.id = people.sickness_id GROUP BY person_id\")\n",
    "youngest_animal = c.fetchone()\n",
    "youngest_animal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's try to structure some information into a predefined Json structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 2048 tokens, input is 370 tokens\n",
      "<|im_start|>assistent\n",
      "You are an expert JSON chatbot. Generate ONLY JSON structure besed on user requests and there story. you do not provide any other information.<|im_end|>\n",
      "<|im_start|>user\n",
      " I have a JSON structure: \n",
      "{\n",
      "  \"name\": string,\n",
      "  \"age\": integer,\n",
      "  \"city\": string,\n",
      "  \"meet\": string,\n",
      "}\n",
      ". During my travels, I had the pleasure of meeting a remarkable person named John. He is a 45-year-old New Yorker with a wealth of stories and experiences that make him truly fascinating. Our paths crossed in a quaint little café tucked away in one of the quieter streets of Manhattan. It was one of those places where the aroma of freshly brewed coffee blends harmoniously with the chatter of locals, creating an ambiance that invites you to sit back and savor the moment. Can you give me the JSON that folows the structure?<|im_end|>\n",
      "<|im_start|>assistent\n",
      "{\"name\": \"John\", \"age\": 45, \"city\": \"New York\", \"meet\": \"café\"}<|im_end|>\n",
      "<|im_start|>user\n",
      " I have a JSON structure: \n",
      "{\n",
      "  \"name\": string,\n",
      "  \"age\": integer,\n",
      "  \"city\": string,\n",
      "  \"meet\": string,\n",
      "}\n",
      ". During the summer, in a small village in the south of France called Saint-Tropez, I met a charming lady named Marie. She is 32 years old and has a passion for painting. Her art is a reflection of her vibrant personality and zest for life. We met at a local art exhibition, where her work was on display. The colors and textures of her paintings captivated me, and I was drawn to the stories they told. Can you give me the JSON that folows the structure?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "{\"name\": \"Marie\", \"age\": 32, \"city\": \"Saint-Tropez\", \"meet\": \"art exhibition\", \"work\": \"painting\"}<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "json_structure = \"\"\"\n",
    "{\n",
    "  \"name\": string,\n",
    "  \"age\": integer,\n",
    "  \"city\": string,\n",
    "  \"meet\": string,\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "message = [{\"role\": \"assistent\", \"content\": \"You are an expert JSON chatbot. Generate ONLY JSON structure besed on user requests and there story. you do not provide any other information.\"},\n",
    "           {\"role\": \"user\", \"content\": f\" I have a JSON structure: {json_structure}. During my travels, I had the pleasure of meeting a remarkable person named John. He is a 45-year-old New Yorker with a wealth of stories and experiences that make him truly fascinating. Our paths crossed in a quaint little café tucked away in one of the quieter streets of Manhattan. It was one of those places where the aroma of freshly brewed coffee blends harmoniously with the chatter of locals, creating an ambiance that invites you to sit back and savor the moment. Can you give me the JSON that folows the structure?\"},\n",
    "           {\"role\": \"assistent\", \"content\": '{\"name\": \"John\", \"age\": 45, \"city\": \"New York\", \"meet\": \"café\"}'},\n",
    "           {\"role\": \"user\", \"content\": f\" I have a JSON structure: {json_structure}. During the summer, in a small village in the south of France called Saint-Tropez, I met a charming lady named Marie. She is 32 years old and has a passion for painting. Her art is a reflection of her vibrant personality and zest for life. We met at a local art exhibition, where her work was on display. The colors and textures of her paintings captivated me, and I was drawn to the stories they told. Can you give me the JSON that folows the structure?\"},\n",
    "           ]\n",
    "\n",
    "input_text=inst_tokenizer.apply_chat_template(message, tokenize=False)\n",
    "\n",
    "output = generated_text(input_text, tokenizer=inst_tokenizer, model=inst_model, config=inst_config, max_new_tokens=100)\n",
    "\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
